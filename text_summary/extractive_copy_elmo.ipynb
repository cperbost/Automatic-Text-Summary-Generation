{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extractive+copy+elmo.ipynb","version":"0.3.2","provenance":[{"file_id":"1hX23oHmLfpCbLD3BXTeoysLxyLf5DLE3","timestamp":1557358947839},{"file_id":"1R11OxA37XjWYh3JJo1i1gMfyWBxhRNSc","timestamp":1557293544204},{"file_id":"1eU6nuSPPlxPyP8w_Dr_1yoyDRwI6c-06","timestamp":1557027782379},{"file_id":"1xjdknvs8jOc33P1lVpwOBiDMCD_Jw5Py","timestamp":1557008778037},{"file_id":"18Ye9CF1feEqECdVEiTa-Tnn_sIsZ-TRX","timestamp":1555259810816},{"file_id":"1SNNZDOXYYZ4mkbMBW6yTghgkTb9CY6fh","timestamp":1554915703195}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vud6621Z0oiX","colab_type":"code","outputId":"d66f2453-c1f9-454e-d9e1-58f7522abf28","executionInfo":{"status":"ok","timestamp":1557362727496,"user_tz":240,"elapsed":13500,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["!pip install PyDrive\n","!pip install rouge"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K     |████████████████████████████████| 993kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n","Collecting rouge\n","  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n","Installing collected packages: rouge\n","Successfully installed rouge-0.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umtkYzVauQRY","colab_type":"code","colab":{}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import json, gzip\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import time\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from google.colab import drive\n","from oauth2client.client import GoogleCredentials\n","import pickle\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAceJTFO3yua","colab_type":"code","outputId":"ee83daa5-6f96-49ea-895d-712c7638e4d6","executionInfo":{"status":"ok","timestamp":1557362767134,"user_tz":240,"elapsed":684,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","print(torch.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n","1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4iGfx_Xv0xTj","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFUmkHfU05pX","colab_type":"code","colab":{}},"source":["#Data for training set = 10k dev set = 1k\n","download = drive.CreateFile({'id': '1yJB36Jj1jw3U0JwE3IwhBYDY3VI1oCGd'})\n","download.GetContentFile('train.p')\n","download = drive.CreateFile({'id': '1T6YxqByvjNlCSZrU7FVmdCYlxM0RHlgw'})\n","download.GetContentFile('dev.p')\n","download = drive.CreateFile({'id': '1Nja8rXISqKfTSEbIPKYaf6Ey_Ud_Fik7'})\n","download.GetContentFile('test.p')\n","download = drive.CreateFile({'id': '1vPOGkgumTOVQ4N84GBo8AAPeAgwfyMep'})\n","download.GetContentFile('index2word.p')\n","download = drive.CreateFile({'id': '1JRYv-RQqhX6-fdYVUmyZmP9tBOtZjp_w'})\n","download.GetContentFile('word2index.p')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wm74RC5_1UYH","colab_type":"code","colab":{}},"source":["# #Data for training set = 50k dev set = 5k\n","# download = drive.CreateFile({'id': '1xq4JlZoWZ8_eO5F6VNBQvk2Mvub1Etpb'})\n","# download.GetContentFile('trainL.p')\n","# download = drive.CreateFile({'id': '1G3YAtOtrfv3PswNSHyGm0SIZy60n95zU'})\n","# download.GetContentFile('devL.p')\n","# download = drive.CreateFile({'id': '1T7rj8WokLSEEbc7-nqH48mjSnwBt4hEq'})\n","# download.GetContentFile('testL.p')\n","# download = drive.CreateFile({'id': '1bZXADxMyEn-yWtneA1sxDouGpuce1LRh'})\n","# download.GetContentFile('index2wordL.p')\n","# download = drive.CreateFile({'id': '1IMnW7Oyq4q9Ho5z87MbmVwlODjzEvY9h'})\n","# download.GetContentFile('word2indexL.p')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwxACW1NSqN1","colab_type":"code","colab":{}},"source":["train_dict = pickle.load( open( \"train.p\", \"rb\" ) )\n","dev_dict = pickle.load( open( \"dev.p\", \"rb\" ) )\n","test_dict = pickle.load( open( \"test.p\", \"rb\" ) )\n","index2word = pickle.load( open( \"index2word.p\", \"rb\" ) )\n","word2index = pickle.load( open( \"word2index.p\", \"rb\" ) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqSnGLzQYY2t","colab_type":"code","colab":{}},"source":["train_dict = sorted(train_dict, key = lambda i: len(i['text']), reverse = True)\n","dev_dict = sorted(dev_dict, key = lambda i: len(i['text']), reverse = True)\n","test_dict = sorted(test_dict, key = lambda i: len(i['text']), reverse = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yK4e6UyGC3rh","colab_type":"code","colab":{}},"source":["train_dict = train_dict[:10]\n","dev_dict = dev_dict[:10]\n","test_dict = test_dict[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3boC7yW5C86J","colab_type":"code","outputId":"0a7883f5-1030-4bcc-f9c5-66b0edf47464","executionInfo":{"status":"ok","timestamp":1557362829236,"user_tz":240,"elapsed":463,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["train_dict_text = []\n","for i, data in enumerate(train_dict):\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","#         if i==789:\n","#             print(s, summ)\n","    t['summary'] = summ\n","    train_dict_text.append(t)\n","print(len(train_dict_text))\n","\n","dev_dict_text = []\n","for data in dev_dict:\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","    t['summary'] = summ\n","    dev_dict_text.append(t)\n","print(len(dev_dict_text))\n","\n","test_dict_text = []\n","for data in test_dict:\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","    t['summary'] = summ\n","    test_dict_text.append(t)\n","print(len(test_dict_text))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10\n","10\n","10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SPeyLPNuDAy0","colab_type":"code","outputId":"082bb467-e26f-4d4c-a402-b07531e39d01","executionInfo":{"status":"ok","timestamp":1557362830831,"user_tz":240,"elapsed":395,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["ind = 0\n","print(train_dict_text[ind]['summary'])\n","print(train_dict[ind]['summary'])\n","print(index2word[train_dict[ind]['summary'][-2]])\n","print(test_dict_text[ind]['summary'])\n","print(test_dict[ind]['summary'])\n","print(index2word[test_dict[ind]['summary'][-2]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["bradford city and port vale play out a goalless draw on the opening day of the league one season .\n","[3228, 1932, 18, 3230, 3231, 3082, 199, 13, 3233, 3234, 134, 38, 847, 410, 46, 38, 1189, 405, 963, 50]\n","season\n","a blackout that cut power to all of south australia has prompted thousands to take to social media .\n","[13, 4863, 250, 6296, 3141, 21, 502, 46, 2863, 3383, 52, 13140, 1182, 21, 37, 21, 2143, 11, 50]\n","media\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BmJRAXh8DFrY","colab_type":"code","outputId":"8c335324-f40c-4c80-a566-2cd9d7abd8a7","executionInfo":{"status":"ok","timestamp":1557362834678,"user_tz":240,"elapsed":2789,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import nltk\n","import nltk.data\n","from nltk import tokenize\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import os, sys\n","import pickle\n","import json, gzip\n","import time\n","import math\n","from sklearn.cluster import KMeans\n","import numpy as np\n","from collections import defaultdict\n","from scipy.spatial.distance import euclidean\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0508 20:47:14.214939 139833398060928 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"oEJWJQEoDJec","colab_type":"code","colab":{}},"source":["module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HBmZ-d3DNlV","colab_type":"code","outputId":"bf544a01-df4f-4fcf-b40d-78004b2fd191","executionInfo":{"status":"ok","timestamp":1557362849422,"user_tz":240,"elapsed":12240,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["embed = hub.Module(module_url)\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["W0508 20:47:27.937835 139833398060928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lnBVSwMwDtCu","colab_type":"code","colab":{}},"source":["g = tf.Graph()\n","with g.as_default():\n","    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n","    embed = hub.Module(module_url)\n","    embedded_text = embed(text_input)\n","    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n","g.finalize()\n","\n","session = tf.Session(graph=g)\n","session.run(init_op)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGrYzgTEDvA-","colab_type":"code","outputId":"2d004784-56fb-4e7c-bc01-315928e5baeb","executionInfo":{"status":"ok","timestamp":1557362887148,"user_tz":240,"elapsed":2792,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["ext_train_data = []\n","ext_dev_data = []\n","ext_test_data = []\n","\n","for i, data in enumerate(train_dict_text):\n","    if i%1000==0:\n","        print(i)\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    try:\n","        sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","        num_sent = sentences_embeddings.shape[0]\n","        k = round(math.sqrt(num_sent))\n","        clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","        cluster_centers = clustering.cluster_centers_\n","        center_dist = defaultdict(list)\n","        for ns in range(num_sent):\n","            label = clustering.labels_[ns]\n","            dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","            center_dist[label].append((dist, ns))\n","\n","        final = []\n","        for labels in range(k):\n","            try:\n","                center_dist[labels].sort(key = lambda x: x[0])\n","                final.append(center_dist[labels][0][1])\n","            except:\n","                continue\n","        final.sort()\n","        ex = []\n","        for i in final:\n","            ex.append(sentences_list[i])\n","        extractive = (' ').join(ex)\n","        dict_ = {}\n","        dict_['text'] = extractive[:]\n","        dict_['summary'] = data['summary'][:]\n","        ext_train_data.append(dict_)\n","    except:\n","        print('error')\n","end = time.time()\n","print(end-start)\n","\n","for i, data in enumerate(dev_dict_text):\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    \n","    sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","    num_sent = sentences_embeddings.shape[0]\n","    k = round(math.sqrt(num_sent))\n","    clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","    cluster_centers = clustering.cluster_centers_\n","    center_dist = defaultdict(list)\n","    for ns in range(num_sent):\n","        label = clustering.labels_[ns]\n","        dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","        center_dist[label].append((dist, ns))\n","            \n","    final = []\n","    for labels in range(k):\n","        try:\n","            center_dist[labels].sort(key = lambda x: x[0])\n","            final.append(center_dist[labels][0][1])\n","        except:\n","            continue\n","    final.sort()\n","    ex = []\n","    for i in final:\n","        ex.append(sentences_list[i])\n","    extractive = (' ').join(ex)\n","    dict_ = {}\n","    dict_['text'] = extractive[:]\n","    dict_['summary'] = data['summary'][:]\n","    ext_dev_data.append(dict_)\n","end = time.time()\n","print(end-start)\n","\n","for i, data in enumerate(test_dict_text):\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    \n","    sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","    num_sent = sentences_embeddings.shape[0]\n","    k = round(math.sqrt(num_sent))\n","    clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","    cluster_centers = clustering.cluster_centers_\n","    center_dist = defaultdict(list)\n","    for ns in range(num_sent):\n","        label = clustering.labels_[ns]\n","        dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","        center_dist[label].append((dist, ns))\n","            \n","    final = []\n","    for labels in range(k):\n","        try:\n","            center_dist[labels].sort(key = lambda x: x[0])\n","            final.append(center_dist[labels][0][1])\n","        except:\n","            continue\n","    final.sort()\n","    ex = []\n","    for i in final:\n","        ex.append(sentences_list[i])\n","    extractive = (' ').join(ex)\n","    dict_ = {}\n","    dict_['text'] = extractive[:]\n","    dict_['summary'] = data['summary'][:]\n","    ext_test_data.append(dict_)\n","end = time.time()\n","print(end-start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","0.03281283378601074\n","0.033701419830322266\n","0.03316330909729004\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lNgPncI3D3oA","colab_type":"code","colab":{}},"source":["ext_train_data = sorted(ext_train_data, key = lambda i: len(i['text']), reverse = True)\n","ext_dev_data = sorted(ext_dev_data, key = lambda i: len(i['text']), reverse = True)\n","ext_test_data = sorted(ext_test_data, key = lambda i: len(i['text']), reverse = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgtjSuwcEDe6","colab_type":"code","outputId":"8e73db17-eafb-4e67-ee6f-a6ed68b62812","executionInfo":{"status":"ok","timestamp":1557362891745,"user_tz":240,"elapsed":466,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["ind = 2\n","print(ext_train_data[ind]['text'])\n","print(ext_train_data[ind]['summary'])\n","print(ext_dev_data[ind]['text'])\n","print(ext_dev_data[ind]['summary'])\n","print(ext_test_data[ind]['text'])\n","print(ext_test_data[ind]['summary'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["today the company has announced a partnership with silicon solutions maker marvell that will make that concept a reality by 2011 — and for just $100 per device . marvell co-founder weili dai also told the new york times that the first version of the tablet would be equipped with google’s android operating system . watch the video below for more information on the tablet .\n","one laptop per child has teamed up with silicon solutions maker marvell to create $100 tablets by 2011 .\n","that’s a bit harder , as instagram disabled twitter UNK back in UNK long after facebook bought instagram in a $1-billion deal . to get that bad boy on twitter as an embedded video , simply upload the  UNK to youtube (preferably over wi-fi , as big  UNK files can really eat into your data plan) . once your hyperlapse is up on youtube , simply tweet out the youtube link and your video will appear embedded in the tweet:\n","it 's a youtube-based workaround\n","researchers studied the UNK size of 144 UNK collected during aboriginal UNK UNK between the years 1990 and 2008 , and found that the bigger the UNK , the better the UNK , pretty much — tusk length and UNK mass (which indicates UNK were closely related . that means UNK UNK aren’t unlike peacock feathers or UNK in the sense that they’re animal body parts used to attract females with their impressive displays .\n","attracting the ladies one giant tooth at a time .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wuCMLRoZEL6O","colab_type":"code","colab":{}},"source":["from nltk.tokenize import word_tokenize\n","ind2word = {0: 'PAD', 1:'BOS', 2:'EOS', 3:'UNK'}\n","word2ind = {'PAD':0, 'BOS':1, 'EOS':2, 'UNK':3}\n","count = 4\n","for data in ext_train_data:\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    for words in word_tokenize(text):\n","        if words not in word2ind:\n","            word2ind[words] = count\n","            ind2word[count] = words\n","            count += 1\n","\n","    for words in word_tokenize(summ):\n","        if words not in word2ind:\n","            word2ind[words] = count\n","            ind2word[count] = words\n","            count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyWDw-qJENRm","colab_type":"code","outputId":"1ac7940f-3119-4946-9d4c-0129b4ec09bc","executionInfo":{"status":"ok","timestamp":1557362896280,"user_tz":240,"elapsed":406,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ind = 2\n","# print(ext_train_data[ind]['text'])\n","print(ext_train_data[ind]['summary'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["one laptop per child has teamed up with silicon solutions maker marvell to create $100 tablets by 2011 .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JXLmhlW5ETdV","colab_type":"code","colab":{}},"source":["train_dict = []\n","dev_dict = []\n","train_maxlen = 30\n","dev_maxlen = 30\n","\n","for i, data in enumerate(ext_train_data):\n","    data_point = {}\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    text_ind = []\n","    summ_ind = []\n","    \n","    for words in word_tokenize(summ):\n","        summ_ind.append(word2ind[words])\n","    summ_ind = [1]+summ_ind+[2]\n","    while len(summ_ind)<train_maxlen:\n","        summ_ind.append(0)\n","    if len(summ_ind)==train_maxlen:\n","        data_point['text'] = word_tokenize(text)\n","        data_point['text_len'] = len(data_point['text'])\n","        data_point['summary_ind'] = summ_ind[:]\n","        data_point['summary'] = (['BOS']+word_tokenize(summ)+['EOS'])[:]\n","        train_dict.append(data_point)\n","\n","    \n","    \n","for i, data in enumerate(ext_dev_data):\n","    data_point = {}\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    text_ind = []\n","    summ_ind = []\n","    \n","    for words in word_tokenize(summ):\n","        if words in word2ind:\n","            summ_ind.append(word2ind[words])\n","        else:\n","            summ_ind.append(word2ind['UNK'])\n","    summ_ind = [1]+summ_ind+[2]\n","    if len(summ_ind)<dev_maxlen:\n","        summ_ind += [0]*(dev_maxlen-len(summ_ind))\n","    \n","    if len(summ_ind)==dev_maxlen:\n","        data_point['text'] = word_tokenize(text)\n","        data_point['text_len'] = len(data_point['text'])\n","        data_point['summary_ind'] = summ_ind[:]\n","        data_point['summary'] = (['BOS']+word_tokenize(summ)+['EOS'])[:]\n","        dev_dict.append(data_point)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRPwxGySEWVe","colab_type":"code","outputId":"ba832ebb-779b-458d-814f-687438bb8194","executionInfo":{"status":"ok","timestamp":1557362900040,"user_tz":240,"elapsed":299,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data = {}\n","data['train'] = train_dict[:]\n","data['dev'] = dev_dict[:]\n","data['test'] = test_dict[:]\n","s = (data['dev'][9]['summary_ind'])\n","print(s)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1, 284, 134, 3, 3, 3, 178, 65, 8, 3, 7, 294, 3, 3, 3, 188, 8, 69, 3, 3, 3, 65, 2, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lczPrdl2BNXs","colab_type":"code","colab":{}},"source":["!pip install allennlp\n","from allennlp.modules.elmo import Elmo\n","from allennlp.modules.elmo import batch_to_ids\n","\n","options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n","weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n","\n","elmo = Elmo(options_file, weight_file, 1, dropout=0, requires_grad = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"edrWwinX9mgm","colab_type":"code","colab":{}},"source":["trunc_norm_init_std = 1e-4\n","def init_linear_wt(linear):\n","    linear.weight.data.normal_(std=trunc_norm_init_std)\n","    if linear.bias is not None:\n","        linear.bias.data.normal_(std=trunc_norm_init_std)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNz5eWSczFZ-","colab_type":"code","colab":{}},"source":["class biEncoderLSTM(nn.Module):\n","    \n","    def __init__(self, params):\n","        super(biEncoderLSTM, self).__init__()\n","        self.hidden_size = params['hidden_size']\n","        self.emb_size = params['emb_size']\n","        self.vocab_size = params['vocab_size']\n","        self.num_layers = params['num_layers']\n","        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n","        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True, num_layers=self.num_layers, bidirectional = True)\n","        self.reduce_h = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        init_linear_wt(self.reduce_h)\n","        self.reduce_c = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        init_linear_wt(self.reduce_c)\n","        self.elmo = Elmo(options_file, weight_file, 1, requires_grad=True, dropout=0)\n","\n","    def forward(self, input, hidden, memory, seq_lengths):\n","        batch = len(input)\n","        character_ids = batch_to_ids(input).cuda()\n","        emb = (self.elmo(character_ids)['elmo_representations'][0]).cuda()\n","        #emb = self.embedding(input)\n","        #packed_emb = pack_padded_sequence(emb, (seq_lengths).numpy(), batch_first = True, enforce_sorted=False)\n","        #packed_output, (h, c) = self.lstm(packed_emb, (hidden, memory))\n","        #output, _ = pad_packed_sequence(packed_output, batch_first = True)\n","        output, (h, c) = self.lstm(emb, (hidden, memory))\n","        output1 = output[ :, :, :self.hidden_size]\n","        output2 = output[:, :, self.hidden_size:] #reverse\n","        output = output1+output2\n","        h = torch.cat(list(h), dim=1)                                   \n","        c = torch.cat(list(c), dim=1)\n","        h_reduced = F.relu(self.reduce_h(h))                       \n","        c_reduced = F.relu(self.reduce_c(c))\n","        return output, h_reduced.unsqueeze(dim=0), c_reduced.unsqueeze(dim=0)\n","\n","    def initHidden(self, batch_size):\n","        h_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n","        c_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n","        return h_init, c_init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sso-qAXHlOKX","colab_type":"code","colab":{}},"source":["class AttnDecoderLSTM(nn.Module):\n","    \n","    def __init__(self, params):\n","        super(AttnDecoderLSTM, self).__init__()\n","        self.hidden_size = params['hidden_size']\n","        self.vocab_size = params['vocab_size']\n","        self.emb_size = params['emb_size']\n","        self.num_layers = params['num_layers']\n","        self.batch_size = params['batch_size']\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first=True, num_layers = self.num_layers)\n","        self.w = nn.Linear(2*self.hidden_size, self.vocab_size)\n","        self.p_gen_linear = nn.Linear(self.hidden_size + self.emb_size, 1)\n","        self.elmo = Elmo(options_file, weight_file, 1, requires_grad=True, dropout=0)\n","\n","    def forward(self, input, hidden, memory, encoder_outputs, pad_ind):\n","        pointer_gen = True\n","        batch_size = len(input)\n","        #embedded = self.embedding(input)\n","        character_ids = batch_to_ids(input).cuda()\n","        #embedded = self.embedding(input)\n","        #embedded = self.dropout(embedded)\n","        embedded = (self.elmo(character_ids)['elmo_representations'][0]).cuda()\n","        output,(h,c) = self.lstm(embedded, (hidden, memory))\n","        attn_values = torch.bmm(output,encoder_outputs.permute(0,2,1))\n","        dec_attn_values = torch.bmm(output,output.permute(0,2,1))\n","        for b in range(batch_size):\n","            attn_values[b,:,pad_ind[b]:] = -1e10\n","        mask = torch.ones_like(dec_attn_values).cuda()\n","        mask1 = -1e10*torch.ones_like(dec_attn_values).cuda()\n","        \n","        mask = torch.tril(mask, diagonal = -1)\n","        mask1 = torch.triu(mask1)\n","        mask = mask+mask1\n","        dec_attn_values = dec_attn_values*mask.float()\n","        \n","        total_attn_values = torch.cat((attn_values,dec_attn_values),2)\n","        total_outputs = torch.cat((encoder_outputs,output),1)\n","        \n","        sftmax = nn.Softmax(dim=2)\n","        attn_dist = sftmax(attn_values)# This is attention distribution\n","        \n","        total_attn_weights = sftmax(total_attn_values) \n","        \n","        weight_attn = torch.bmm(total_attn_weights, total_outputs)\n","        final_hidden = torch.cat((output,weight_attn),2)\n","        scores = self.w(final_hidden)\n","        sftmax2 = nn.Softmax(dim=1)\n","        vocab_dist = sftmax2(scores)\n","        \n","        p_gen = None\n","        if pointer_gen:\n","            p_gen_input = torch.cat((embedded, output), 2)  # B x (hidden_dim + emb_dim)\n","            p_gen = self.p_gen_linear(p_gen_input)\n","            p_gen = torch.sigmoid(p_gen)\n","        vocab_dist_ = p_gen * vocab_dist\n","        attn_dist_ = (1 - p_gen) * attn_dist  \n","        return vocab_dist_, attn_dist_\n","\n","\n","    def initHidden(self, batch_size):\n","        h_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n","        c_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n","        return h_init, c_init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"45tYzDjDvxwh","colab_type":"code","colab":{}},"source":["from rouge import Rouge\n","\n","def calculate_rouge (hyps, refs):\n","    #print(hyps, refs)\n","    r = Rouge()\n","    scores = r.get_scores(hyps, refs, avg=True)\n","    return(scores)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipBXkxNyx3UF","colab_type":"code","colab":{}},"source":["def train_lm(dataset, params, net, fscore = False):\n","    \n","    #print(dataset[9920]['summary'])\n","    # since the first index corresponds to the PAD token, we just ignore it\n","    # when computing the loss\n","    criterion = nn.CrossEntropyLoss(ignore_index=0)\n","    sftmax = nn.Softmax(dim=2)\n","    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n","    num_examples = len(dataset)    \n","    batches = [(start, start + params['batch_size']) for start in range(0, num_examples, params['batch_size'])]\n","    \n","    for epoch in range(params['epochs']):\n","        ep_loss = 0.\n","        start_time = time.time()\n","        #random.shuffle(batches)\n","        ref_summ = []\n","        gen_summ = []\n","        count = 0\n","        # for each batch, calculate loss and optimize model parameters            \n","        for b_idx, (start, end) in enumerate(batches):\n","            batch = dataset[start:end]\n","            \n","            batch_text = []\n","            batch_summary = []\n","            batch_pad_len = []\n","#             batch_summ_indexes = []\n","            batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n","            i = 0\n","            for d in batch:\n","                \n","                batch_text.append(d['text'])\n","                batch_summary.append(d['summary'])\n","                batch_pad_len.append(d['text_len'])\n","#                 batch_summ_indexes.append(d['summary_ind'])\n","                batch_summ_indexes[i] = d['summary_ind']\n","                #print(len(d['summary_ind']), len(d['summary']))\n","                \n","                i += 1\n","            \n","            \n","            batch_summ_indexes = torch.from_numpy(batch_summ_indexes).long().cuda()\n","#             batch_summ_indexes = torch.tensor(batch_summ_indexes).long.cuda()\n","            vocab_dist_, attn_dist_ = net(batch_text, batch_summary, batch_pad_len)\n","            _,_,l = attn_dist_.size()\n","            enc_batch_extend_vocab = batch_text.unsqueeze(1)\n","            enc_batch_extend_vocab = enc_batch_extend_vocab[:,:,:l]\n","            \n","            enc_batch_extend_vocab = enc_batch_extend_vocab.expand_as(attn_dist_)\n","            final_dist = vocab_dist_.scatter_add(2, enc_batch_extend_vocab, attn_dist_)\n","            \n","            targets = batch_summary[:, 1:].contiguous()\n","            final_dist = final_dist[:, :-1, :]\n","            gold_probs = torch.gather(final_dist, 2, targets.unsqueeze(2)).squeeze()\n","            nll = -torch.log(gold_probs +  1e-12)\n","            #remove padding loss\n","            for b in range(len(batch)):\n","                nll[b,batch_summ_len[b]-1:] = 0\n","            loss = torch.sum(nll)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            ep_loss += loss\n","            \n","            if fscore:\n","                logits = final_dist\n","                words = torch.argmax(logits, dim=2)\n","                for sent in range(words.size()[0]):\n","                    summ = ''\n","                    gold = ''\n","                    for word in range(words.size()[1]):\n","                        if ind2word[words[sent][word].item()] == 'EOS':\n","                            break\n","                        if summ :\n","                            summ += ' '\n","                        summ += ind2word[words[sent][word].item()]\n","\n","                    for word in range(1,len(batch_summary[sent])-1):\n","                        if ind2word[batch_summary[sent][word].item()] == 'EOS':\n","                            break\n","                        if gold:\n","                            gold += ' '\n","                        gold += ind2word[batch_summary[sent][word].item()]\n","\n","                    if summ == '.'*len(summ):\n","                        count += 1\n","                    if not summ or summ == '.'*len(summ):\n","                        summ = '====='\n","\n","                    gen_summ.append(summ)\n","                    ref_summ.append(gold)\n","                    \n","        idx = 5\n","        logits = final_dist[idx,:,:].unsqueeze(0)\n","        words = torch.argmax(logits, dim=2).squeeze(0)\n","        gold_summ_last = ''\n","        summ_last = ''\n","        for l in range(batch_summary[idx,1:].size()[0]):\n","            gold_summ_last += ind2word[batch_summary[idx,l].item()] + ' '\n","        for l in range(words.size()[0]):\n","            summ_last += ind2word[words[l].item()] + ' '\n","        print('########################Train######################')\n","        print(gold_summ_last)\n","        print(summ_last)\n","        \n","        if fscore:\n","            rouge_score = calculate_rouge(gen_summ, ref_summ)\n","            print('rouge', rouge_score)\n","        \n","        loss, perplex =  compute_loss(data['dev'], net, params['batch_size'], fscore = True)\n","        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev loss: %0.2f, dev perplexity: %0.2f' %\\\n","              (epoch, ep_loss, time.time()-start_time, loss, perplex))\n","        print('####################################################')\n","\n","\n","\n","            pred, = net(batch_text, batch_summary, batch_pad_len)\n","            print((pred))\n","            sz = pred.size()[1]\n","            preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n","            targets = batch_summ_indexes[:, 1:sz].contiguous().view(-1)\n","            #print(pred.size(),preds.size(),targets.size(),batch_summ_indexes.size())\n","            loss = criterion(preds, targets)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            ep_loss += loss\n","            \n","            if fscore:\n","                logits = sftmax(pred)\n","                words = torch.argmax(logits, dim=2)\n","                for sent in range(words.size()[0]):\n","                    summ = ''\n","                    gold = ''\n","                    for word in range(words.size()[1]):\n","                        if ind2word[words[sent][word].item()] == 'EOS':\n","                            break\n","                        if summ :\n","                            summ += ' '\n","                        summ += ind2word[words[sent][word].item()]\n","\n","                    for word in range(1,len(batch_summary[sent])-1):\n","                        if ind2word[batch_summ_indexes[sent][word].item()] == 'EOS':\n","                            break\n","                        if gold:\n","                            gold += ' '\n","                        gold += ind2word[batch_summ_indexes[sent][word].item()]\n","\n","                    if summ == '.'*len(summ):\n","                        count += 1\n","                    if not summ or summ == '.'*len(summ):\n","                        summ = '====='\n","                    if not gold or gold=='.'*len(gold):\n","                        continue\n","\n","                    gen_summ.append(summ)\n","                    ref_summ.append(gold)\n","        print(start)   \n","        ind = 0\n","        scores = pred[ind,:,:].unsqueeze(0)\n","        logits = sftmax(scores)\n","        words = torch.argmax(logits, dim=2).squeeze(0)\n","        gold_summ_last = ''\n","        summ_last = ''\n","        for l in range(batch_summ_indexes[ind,1:].size()[0]):\n","            gold_summ_last += ind2word[batch_summ_indexes[ind,l].item()] + ' '\n","        for l in range(words.size()[0]):\n","            summ_last += ind2word[words[l].item()] + ' '\n","        print('########################Train######################')\n","        print(gold_summ_last)\n","        print(summ_last)\n","        \n","        #print(len(gen_summ), len(ref_summ))\n","        if fscore:\n","            rouge_score = calculate_rouge(gen_summ, ref_summ)\n","            print('rouge', rouge_score)\n","        \n","#         print('epoch: %d, loss: %0.2f, time: %0.2f sec' %\\\n","#                 (epoch, ep_loss, time.time()-start_time))\n","        loss, perplex =  compute_loss(data['dev'], net, params['batch_size'], fscore = True)\n","        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev loss: %0.2f, dev perplexity: %0.2f' %\\\n","              (epoch, ep_loss, time.time()-start_time, loss, perplex))\n","        print('####################################################')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqI9kbh7Sz2Z","colab_type":"code","colab":{}},"source":["def compute_loss(dataset, net, bsz=10, fscore = False):\n","    \n","    sftmax= nn.Softmax(dim=2)\n","    criterion = nn.CrossEntropyLoss(ignore_index=0)\n","    num_examples = len(dataset)  \n","    # we'll still use batches b/c we can't fit the whole\n","    # validation set into GPU memory\n","    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n","    \n","    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n","    nll = 0.\n","    gen_summ = []\n","    ref_summ = []\n","    count = 0\n","    for b_idx, (start, end) in enumerate(batches):\n","#         print(start, end)\n","        batch = dataset[start:end]\n","    \n","        batch_text = []\n","        batch_summary = []\n","        batch_pad_len = []\n","        batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n","        i = 0\n","        for d in batch:\n","            batch_text.append(d['text'])\n","            batch_summary.append(d['summary'])\n","            batch_pad_len.append(d['text_len'])\n","            batch_summ_indexes[i] = d['summary_ind']\n","            i += 1\n","                \n","        #batch_text = torch.from_numpy(np.array(batch_text)).long().cuda()\n","        #batch_summary = torch.from_numpy(np.array(batch_summary)).long().cuda()\n","        batch_pad_len = torch.from_numpy(np.array(batch_pad_len)).long().cuda()\n","        batch_summ_indexes = torch.from_numpy(batch_summ_indexes).long().cuda()\n","#        ut = torch.nonzero(batch_text).size(0)\n","        pred = net(batch_text, batch_summary, batch_pad_len)\n","        preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n","        sz = pred.size()[1]\n","        targets = batch_summ_indexes[:, 1:sz].contiguous().view(-1)\n","        loss = criterion(preds, targets)\n","        nll += loss.detach()\n"," #       total_unmasked_tokens += ut\n","        \n","        if fscore:\n","            #print('pred', pred.size())\n","            logits = sftmax(pred)\n","            words = torch.argmax(logits, dim=2)\n","            #print('words', words.size())\n","            for sent in range(words.size()[0]):\n","                summ = ''\n","                gold = ''\n","                for word in range(words.size()[1]):\n","                    if ind2word[words[sent][word].item()] == 'EOS':\n","                        break\n","                    if summ :\n","                        summ += ' '\n","                    summ += ind2word[words[sent][word].item()]\n","\n","                for word in range(1,len(batch_summ_indexes[sent])-1):\n","                    if ind2word[batch_summ_indexes[sent][word].item()] == 'EOS':\n","                        break\n","                    if gold:\n","                        gold += ' '\n","                    gold += ind2word[batch_summ_indexes[sent][word].item()]\n","\n","                if summ == '.'*len(summ):\n","                    count += 1\n","                if not summ or summ == '.'*len(summ):\n","                    summ = '====='\n","                if not gold or gold=='.'*len(gold):\n","                    continue\n","\n","                gen_summ.append(summ)\n","                ref_summ.append(gold)\n","        \n","    ind = 0\n","    scores = pred[ind,:,:].unsqueeze(0)\n","    logits = sftmax(scores)\n","    words = torch.argmax(logits, dim=2).squeeze(0)\n","    gold_summ = ''\n","    summ = ''\n","    for l in range(batch_summ_indexes[ind,1:].size()[0]):\n","        gold_summ += ind2word[batch_summ_indexes[ind,l].item()] + ' '\n","    for l in range(words.size()[0]):\n","        summ += ind2word[words[l].item()] + ' '\n","            \n","    \n","    print('#############Dev###########')\n","    if fscore:\n","        rouge_score = calculate_rouge(gen_summ, ref_summ)\n","        print('rouge', rouge_score)\n","    print(gold_summ)\n","    print(summ)\n","    \n","    perplexity = torch.exp(nll).cpu() #torch.exp(nll / total_unmasked_tokens).cpu()\n","    return nll, perplexity.data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvpUpkENp8nd","colab_type":"code","colab":{}},"source":["# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n","def test_lm(dataset, net, bsz=1, beam_search = False, max_len = 30):\n","    \n","    sftmax = nn.Softmax(dim=1)\n","    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n","    num_examples = len(dataset)  \n","    # we'll still use batches b/c we can't fit the whole\n","    # validation set into GPU memory\n","    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n","    nll = 0\n","    ans = []\n","    for b_idx, (start, end) in enumerate(batches):\n","            \n","        batch = dataset[start:end]\n","        batch_text = []\n","        batch_summary = []\n","        batch_pad_len = []\n","        batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n","        i = 0\n","        \n","        for d in batch:\n","                batch_text.append(d['text'])\n","                batch_summary.append(d['summary'])\n","                batch_pad_len.append(d['text_len'])\n","                batch_summ_indexes[i] = d['summary_ind']\n","                i += 1\n","                \n","#         batch_text = torch.from_numpy(batch_text).long().cuda()\n","#         batch_summary = torch.from_numpy(batch_summary).long().cuda()\n","#         batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n","        \n","#         batch_summary_gen = torch.ones([bsz, 1], dtype=torch.long).cuda()\n","#         bos = ['BOS']\n","        batch_summary_gen = [['BOS'] for i in range(bsz)]\n","        \n","        for i in range(max_len):    \n","            preds = net(batch_text, batch_summary_gen, batch_pad_len)[:,-1,:]\n","            distribution = sftmax(preds)\n","            ind = torch.argmax(distribution,1)\n","            for j in range(bsz):\n","              batch_summary_gen[j].append(index2word[ind[j].item()])\n","        \n","        ans+=batch_summary_gen.copy()\n","    \n","    gen_summ = []\n","    gold_summ = []\n","    \n","    for n in range(num_examples):\n","        summ = ''\n","#         summ = (' ').join(ans[n])\n","        for k in ans[n][1:]:\n","          if k == 'EOS':\n","            break\n","          summ += k + ' '  \n","        gen_summ.append(summ)\n","        \n","    for n in range(num_examples):\n","        summ = ''\n","#         summ = (' ').join(dataset[n]['summary'])\n","        for k in dataset[n]['summary'][1:]:\n","          if k == 'EOS':\n","            break\n","          summ += k + ' '\n","        gold_summ.append(summ)\n","    print((gold_summ))\n","    print((gen_summ))\n","    rouge_score = calculate_rouge(gen_summ, gold_summ)\n","    print('rouge', rouge_score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4nSu01v0L5n","colab_type":"code","colab":{}},"source":["class SuperAwesome(nn.Module):\n","    def __init__(self, params):\n","        super(SuperAwesome, self).__init__()\n","        self.bienc = biEncoderLSTM(params).cuda()\n","        self.attn_dc = AttnDecoderLSTM(params).cuda()\n","        self.vocab_size = params['vocab_size']\n","        #self.bienc.embedding.weight = self.attn_dc.embedding.weight\n","    def forward(self, batch_text, batch_summary, batch_pad_len):\n","        batch_size = len(batch_text)\n","        h_init,c_init = self.bienc.initHidden(batch_size)\n","        enc_out, h_enc, c_enc = self.bienc(batch_text,h_init,c_init, batch_pad_len)\n","        h_init, c_init = h_enc.contiguous(),c_enc.contiguous()\n","        vocab_dist_, attn_dist_ = self.attn_dc(batch_summary,h_init,c_init,enc_out, batch_pad_len)\n","        return vocab_dist_, attn_dist_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeztNH8Ezs5F","colab_type":"code","colab":{}},"source":["params = {}\n","params['hidden_size'] = 200\n","params['emb_size'] = 1024\n","params['num_layers'] = 1\n","params['batch_size'] = 50\n","params['vocab_size'] = len(word2ind)\n","params['learning_rate'] = 0.01\n","params['epochs'] = 30\n","net = SuperAwesome(params)\n","\n","train_lm(data['train'], params, net, fscore = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7wYWyGfres8","colab_type":"code","colab":{}},"source":["test_len = []\n","#TEST TIME\n","gen_summ = []\n","ref_summ = []\n","for i in range(len(data['test'])):\n","    #print(i, end=',')\n","    y = data['test'][i]['summary']\n","    gold = ''\n","    for t in y:\n","        if(ind2word[t]=='SOS'):\n","            continue\n","        if(ind2word[t] == 'EOS'):\n","            break  \n","        gold += (ind2word[t])\n","        gold +=' '\n","    \n","    \n","    print(\"ground truth\")  \n","    print(gold)\n","    net.eval()\n","    summ, l = test_lm(data['test'][i:i+1], net) \n","    test_len.append(l)\n","    if summ == '.'*len(summ):\n","        count += 1\n","    if not summ or summ == '.'*len(summ):\n","        summ = '====='\n","    print(summ)\n","    gen_summ.append(summ)\n","    ref_summ.append(gold)\n","rouge_score = calculate_rouge(gen_summ, ref_summ)\n","print('Test rouge', rouge_score)  \n","# for n in range(num_examples):\n","#         summ = ''\n","#         for i in range(1,max_len+1):\n","#             if summ:\n","#                 summ += ' '\n","#             if int(ans[n,i]) == 2:\n","#                 break\n","#             summ += ind2word[int(ans[n,i])]\n","#         gen_summ.append(summ)"],"execution_count":0,"outputs":[]}]}