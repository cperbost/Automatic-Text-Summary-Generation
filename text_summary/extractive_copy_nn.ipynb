{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extractive+copy+nn.ipynb","version":"0.3.2","provenance":[{"file_id":"1R11OxA37XjWYh3JJo1i1gMfyWBxhRNSc","timestamp":1557293544204},{"file_id":"1eU6nuSPPlxPyP8w_Dr_1yoyDRwI6c-06","timestamp":1557027782379},{"file_id":"1xjdknvs8jOc33P1lVpwOBiDMCD_Jw5Py","timestamp":1557008778037},{"file_id":"18Ye9CF1feEqECdVEiTa-Tnn_sIsZ-TRX","timestamp":1555259810816},{"file_id":"1SNNZDOXYYZ4mkbMBW6yTghgkTb9CY6fh","timestamp":1554915703195}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vud6621Z0oiX","colab_type":"code","outputId":"1f2eee7d-ca80-444a-915b-320e1c4e7705","executionInfo":{"status":"ok","timestamp":1557452919796,"user_tz":240,"elapsed":6376,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["!pip install PyDrive\n","!pip install rouge"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n","Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (0.3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umtkYzVauQRY","colab_type":"code","colab":{}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import json, gzip\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import time\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from google.colab import drive\n","from oauth2client.client import GoogleCredentials\n","import pickle\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAceJTFO3yua","colab_type":"code","outputId":"4c27f18f-9636-40ab-87be-0aab52a10a2d","executionInfo":{"status":"ok","timestamp":1557452927148,"user_tz":240,"elapsed":516,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","print(torch.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cuda:0\n","1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4iGfx_Xv0xTj","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFUmkHfU05pX","colab_type":"code","colab":{}},"source":["# #Data for training set = 10k dev set = 1k\n","# download = drive.CreateFile({'id': '1yJB36Jj1jw3U0JwE3IwhBYDY3VI1oCGd'})\n","# download.GetContentFile('train.p')\n","# download = drive.CreateFile({'id': '1T6YxqByvjNlCSZrU7FVmdCYlxM0RHlgw'})\n","# download.GetContentFile('dev.p')\n","# download = drive.CreateFile({'id': '1Nja8rXISqKfTSEbIPKYaf6Ey_Ud_Fik7'})\n","# download.GetContentFile('test.p')\n","# download = drive.CreateFile({'id': '1vPOGkgumTOVQ4N84GBo8AAPeAgwfyMep'})\n","# download.GetContentFile('index2word.p')\n","# download = drive.CreateFile({'id': '1JRYv-RQqhX6-fdYVUmyZmP9tBOtZjp_w'})\n","# download.GetContentFile('word2index.p')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RA_Q7G1V7SCb","colab_type":"code","colab":{}},"source":["# #Data with Extractive/Abstractive lable\n","download = drive.CreateFile({'id': '1LOzhGMuW8F4ChPUsycBjEOwpLDQPeD0I'})\n","download.GetContentFile('trainNew.p')\n","download = drive.CreateFile({'id': '1yoaCyqZgZ5QszX0g4ABnNV-9p-D6DUlZ'})\n","download.GetContentFile('devNew.p')\n","download = drive.CreateFile({'id': '1XSaEgXAwPWTojc6ZjzIy8vR38jjiCUSW'})\n","download.GetContentFile('testNew.p')\n","download = drive.CreateFile({'id': '1U45R7EOqcJCUQAyHMMRmhFgWoxfzxyL8'})\n","download.GetContentFile('index2wordNew.p')\n","download = drive.CreateFile({'id': '1DGnwq99nJAq9i9TQ45HPjaO3aYWqDDEL'})\n","download.GetContentFile('word2indexNew.p')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YifaMAo47blB","colab_type":"code","colab":{}},"source":["train_dict = pickle.load( open( \"trainNew.p\", \"rb\" ) )\n","dev_dict = pickle.load( open( \"devNew.p\", \"rb\" ) )\n","test_dict = pickle.load( open( \"testNew.p\", \"rb\" ) )\n","index2word = pickle.load( open( \"index2wordNew.p\", \"rb\" ) )\n","word2index = pickle.load( open( \"word2indexNew.p\", \"rb\" ) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4aWQ1eN7g1v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"952008f6-d265-42c0-d185-11a31ae08e5e","executionInfo":{"status":"ok","timestamp":1557452973289,"user_tz":240,"elapsed":359,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["# data_code = \"extractive\"\n","data_code = \"abstractive\"\n","# data_code = \"mixed\"\n","filtered_train_dict = []\n","filtered_dev_dict = []\n","filtered_test_dict = []\n","\n","for ind in range(len(train_dict)):\n","    data = train_dict[ind]\n","    if data['density_bin'] == data_code:\n","        filtered_train_dict.append(data)\n","for ind in range(len(dev_dict)):\n","    data = dev_dict[ind]\n","    if data['density_bin'] == data_code:\n","        filtered_dev_dict.append(data)\n","for ind in range(len(test_dict)):\n","    data = test_dict[ind]\n","    if data['density_bin'] == data_code:\n","        filtered_test_dict.append(data)\n","\n","print(len(filtered_train_dict))\n","print(len(filtered_dev_dict))\n","print(len(filtered_test_dict))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["5903\n","574\n","609\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VODuENtI7nzB","colab_type":"code","colab":{}},"source":["train_dict = filtered_train_dict[:]\n","dev_dict = filtered_dev_dict[:]\n","test_dict = filtered_test_dict[:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwxACW1NSqN1","colab_type":"code","colab":{}},"source":["# train_dict = pickle.load( open( \"train.p\", \"rb\" ) )\n","# dev_dict = pickle.load( open( \"dev.p\", \"rb\" ) )\n","# test_dict = pickle.load( open( \"test.p\", \"rb\" ) )\n","# index2word = pickle.load( open( \"index2word.p\", \"rb\" ) )\n","# word2index = pickle.load( open( \"word2index.p\", \"rb\" ) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqSnGLzQYY2t","colab_type":"code","colab":{}},"source":["train_dict = sorted(train_dict, key = lambda i: len(i['text']), reverse = True)\n","dev_dict = sorted(dev_dict, key = lambda i: len(i['text']), reverse = True)\n","test_dict = sorted(test_dict, key = lambda i: len(i['text']), reverse = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yK4e6UyGC3rh","colab_type":"code","colab":{}},"source":["train_dict = train_dict[:]\n","dev_dict = dev_dict[:]\n","test_dict = test_dict[:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3boC7yW5C86J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"c3477977-e9a8-45de-e538-1145fae807bb","executionInfo":{"status":"ok","timestamp":1557452981675,"user_tz":240,"elapsed":890,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["train_dict_text = []\n","for i, data in enumerate(train_dict):\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","#         if i==789:\n","#             print(s, summ)\n","    t['summary'] = summ\n","    train_dict_text.append(t)\n","print(len(train_dict_text))\n","\n","dev_dict_text = []\n","for data in dev_dict:\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","    t['summary'] = summ\n","    dev_dict_text.append(t)\n","print(len(dev_dict_text))\n","\n","test_dict_text = []\n","for data in test_dict:\n","    t = {}\n","    text_ind = data['text'][:]\n","    summ_ind = data['summary'][:]\n","    text = ''\n","    summ = ''\n","    for txt in text_ind:\n","        if text:\n","            text += ' '\n","        text += index2word[txt]\n","    t['text'] = text\n","    for s in summ_ind:\n","        if summ:\n","            summ += ' '\n","        summ += index2word[s]\n","    t['summary'] = summ\n","    test_dict_text.append(t)\n","print(len(test_dict_text))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["5903\n","574\n","609\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SPeyLPNuDAy0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"c23dd331-7810-4c3b-9ddd-9855918ce872","executionInfo":{"status":"ok","timestamp":1557452985162,"user_tz":240,"elapsed":376,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["ind = 0\n","print(train_dict[ind]['summary'])\n","print(train_dict_text[ind]['summary'])\n","print(index2word[train_dict[ind]['summary'][-2]])\n","print(test_dict[ind]['summary'])\n","print(test_dict_text[ind]['summary'])\n","print(index2word[test_dict[ind]['summary'][-2]])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[9938, 1003, 1215, 370, 38, 75273, 46, 9420]\n","inspiration can come from the unlikeliest of places\n","of\n","[13, 4663, 239, 6050, 3003, 21, 489, 46, 2728, 3229, 52, 12668, 1146, 21, 37, 21, 2063, 11, 50]\n","a blackout that cut power to all of south australia has prompted thousands to take to social media .\n","media\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BmJRAXh8DFrY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"5cf3b2f9-6f5b-4362-8a01-d8b1ec89695b","executionInfo":{"status":"ok","timestamp":1557452989750,"user_tz":240,"elapsed":3089,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["import nltk\n","import nltk.data\n","from nltk import tokenize\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import os, sys\n","import pickle\n","import json, gzip\n","import time\n","import math\n","from sklearn.cluster import KMeans\n","import numpy as np\n","from collections import defaultdict\n","from scipy.spatial.distance import euclidean\n","nltk.download('punkt')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0509 21:49:48.916234 140140726740864 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"oEJWJQEoDJec","colab_type":"code","colab":{}},"source":["module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HBmZ-d3DNlV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"b0b42991-1eaa-4d7f-da87-126191348c04","executionInfo":{"status":"ok","timestamp":1557453008086,"user_tz":240,"elapsed":14627,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["embed = hub.Module(module_url)\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["W0509 21:50:07.468341 140140726740864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lnBVSwMwDtCu","colab_type":"code","colab":{}},"source":["g = tf.Graph()\n","with g.as_default():\n","    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n","    embed = hub.Module(module_url)\n","    embedded_text = embed(text_input)\n","    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n","g.finalize()\n","\n","session = tf.Session(graph=g)\n","session.run(init_op)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGrYzgTEDvA-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"26e43ad2-2bd5-4ddf-e94a-79e0674a9187","executionInfo":{"status":"ok","timestamp":1557453298454,"user_tz":240,"elapsed":219613,"user":{"displayName":"Shyla Gangwar","photoUrl":"","userId":"17693230568302739601"}}},"source":["ext_train_data = []\n","ext_dev_data = []\n","ext_test_data = []\n","\n","for i, data in enumerate(train_dict_text):\n","    if i%1000==0:\n","        print(i)\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    try:\n","        sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","        num_sent = sentences_embeddings.shape[0]\n","        k = round(math.sqrt(num_sent))\n","        clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","        cluster_centers = clustering.cluster_centers_\n","        center_dist = defaultdict(list)\n","        for ns in range(num_sent):\n","            label = clustering.labels_[ns]\n","            dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","            center_dist[label].append((dist, ns))\n","\n","        final = []\n","        for labels in range(k):\n","            try:\n","                center_dist[labels].sort(key = lambda x: x[0])\n","                final.append(center_dist[labels][0][1])\n","            except:\n","                continue\n","        final.sort()\n","        ex = []\n","        for i in final:\n","            ex.append(sentences_list[i])\n","        extractive = (' ').join(ex)\n","        dict_ = {}\n","        dict_['text'] = extractive[:]\n","        dict_['summary'] = data['summary'][:]\n","        ext_train_data.append(dict_)\n","    except:\n","        print('error')\n","end = time.time()\n","print(end-start)\n","\n","for i, data in enumerate(dev_dict_text):\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    \n","    sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","    num_sent = sentences_embeddings.shape[0]\n","    k = round(math.sqrt(num_sent))\n","    clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","    cluster_centers = clustering.cluster_centers_\n","    center_dist = defaultdict(list)\n","    for ns in range(num_sent):\n","        label = clustering.labels_[ns]\n","        dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","        center_dist[label].append((dist, ns))\n","            \n","    final = []\n","    for labels in range(k):\n","        try:\n","            center_dist[labels].sort(key = lambda x: x[0])\n","            final.append(center_dist[labels][0][1])\n","        except:\n","            continue\n","    final.sort()\n","    ex = []\n","    for i in final:\n","        ex.append(sentences_list[i])\n","    extractive = (' ').join(ex)\n","    dict_ = {}\n","    dict_['text'] = extractive[:]\n","    dict_['summary'] = data['summary'][:]\n","    ext_dev_data.append(dict_)\n","end = time.time()\n","print(end-start)\n","\n","for i, data in enumerate(test_dict_text):\n","    start = time.time()\n","    text = data['text'][:]\n","    sentences_list = tokenize.sent_tokenize(text)\n","    #print((sentences_list))\n","    \n","    sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n","    num_sent = sentences_embeddings.shape[0]\n","    k = round(math.sqrt(num_sent))\n","    clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n","    cluster_centers = clustering.cluster_centers_\n","    center_dist = defaultdict(list)\n","    for ns in range(num_sent):\n","        label = clustering.labels_[ns]\n","        dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n","        center_dist[label].append((dist, ns))\n","            \n","    final = []\n","    for labels in range(k):\n","        try:\n","            center_dist[labels].sort(key = lambda x: x[0])\n","            final.append(center_dist[labels][0][1])\n","        except:\n","            continue\n","    final.sort()\n","    ex = []\n","    for i in final:\n","        ex.append(sentences_list[i])\n","    extractive = (' ').join(ex)\n","    dict_ = {}\n","    dict_['text'] = extractive[:]\n","    dict_['summary'] = data['summary'][:]\n","    ext_test_data.append(dict_)\n","end = time.time()\n","print(end-start)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","error\n","0.012334108352661133\n","0.01705145835876465\n","0.018419742584228516\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lNgPncI3D3oA","colab_type":"code","colab":{}},"source":["ext_train_data = sorted(ext_train_data, key = lambda i: len(i['text']), reverse = True)\n","ext_dev_data = sorted(ext_dev_data, key = lambda i: len(i['text']), reverse = True)\n","ext_test_data = sorted(ext_test_data, key = lambda i: len(i['text']), reverse = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgtjSuwcEDe6","colab_type":"code","colab":{}},"source":["ind = 2\n","print(ext_train_data[ind]['text'])\n","print(ext_train_data[ind]['summary'])\n","print(ext_dev_data[ind]['text'])\n","print(ext_dev_data[ind]['summary'])\n","print(ext_test_data[ind]['text'])\n","print(ext_test_data[ind]['summary'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuCMLRoZEL6O","colab_type":"code","colab":{}},"source":["from nltk.tokenize import word_tokenize\n","ind2word = {0: 'PAD', 1:'BOS', 2:'EOS', 3:'UNK'}\n","word2ind = {'PAD':0, 'BOS':1, 'EOS':2, 'UNK':3}\n","count = 4\n","for data in ext_train_data:\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    for words in word_tokenize(text):\n","        if words not in word2ind:\n","            word2ind[words] = count\n","            ind2word[count] = words\n","            count += 1\n","\n","    for words in word_tokenize(summ):\n","        if words not in word2ind:\n","            word2ind[words] = count\n","            ind2word[count] = words\n","            count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyWDw-qJENRm","colab_type":"code","colab":{}},"source":["ind = 2\n","# print(ext_train_data[ind]['text'])\n","print(ext_train_data[ind]['summary'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXLmhlW5ETdV","colab_type":"code","colab":{}},"source":["train_dict = []\n","dev_dict = []\n","test_dict = []\n","\n","for i, data in enumerate(ext_train_data):\n","    data_point = {}\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    text_ind = []\n","    summ_ind = []\n","    \n","    for words in word_tokenize(text):\n","        text_ind.append(word2ind[words])\n","    data_point['text_len'] = len(text_ind)\n","    if len(text_ind)<200:\n","        text_ind += [0]*(200-len(text_ind))\n","        \n","    for words in word_tokenize(summ):\n","        summ_ind.append(word2ind[words])\n","    summ_ind = [1]+summ_ind+[2]\n","    data_point['summ_len'] = len(summ_ind)\n","    if len(summ_ind)<30:\n","        summ_ind += [0]*(30-len(summ_ind))\n","        \n","    if len(summ_ind)>30:\n","        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', i)\n","    else:\n","        data_point['text'] = text_ind[:]\n","        data_point['summary'] = summ_ind[:]\n","        #print((data_point['text_len']))\n","        train_dict.append(data_point)\n","    \n","for data in ext_dev_data:\n","    data_point = {}\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    text_ind = []\n","    summ_ind = []\n","    for words in word_tokenize(text):\n","        if words in word2ind:\n","            text_ind.append(word2ind[words])\n","        else:\n","            text_ind.append(word2ind['UNK'])\n","    data_point['text_len'] = len(text_ind)\n","    if len(text_ind)<200:\n","        text_ind += [0]*(200-len(text_ind))\n","    for words in word_tokenize(summ):\n","        if words in word2ind:\n","            summ_ind.append(word2ind[words])\n","        else:\n","            summ_ind.append(word2ind['UNK'])\n","    summ_ind = [1]+summ_ind+[2]\n","    data_point['summ_len'] = len(summ_ind)\n","    if len(summ_ind)<30:\n","        summ_ind += [0]*(30-len(summ_ind))\n","    if len(summ_ind)>30:\n","        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', i)\n","    else:\n","        data_point['text'] = text_ind[:]\n","        data_point['summary'] = summ_ind[:]\n","        #print((data_point['text_len']))\n","        dev_dict.append(data_point)\n","        \n","for data in ext_test_data:\n","    data_point = {}\n","    text = data['text'][:]\n","    summ = data['summary'][:]\n","    text_ind = []\n","    summ_ind = []\n","    for words in word_tokenize(text):\n","        if words in word2ind:\n","            text_ind.append(word2ind[words])\n","        else:\n","            text_ind.append(word2ind['UNK'])\n","    data_point['text_len'] = len(text_ind)\n","    if len(text_ind)<200:\n","        text_ind += [0]*(200-len(text_ind))\n","    for words in word_tokenize(summ):\n","        if words in word2ind:\n","            summ_ind.append(word2ind[words])\n","        else:\n","            summ_ind.append(word2ind['UNK'])\n","    summ_ind = [1]+summ_ind+[2]\n","    data_point['summ_len'] = len(summ_ind)\n","    if len(summ_ind)<30:\n","        summ_ind += [0]*(30-len(summ_ind))\n","    if len(summ_ind)>30:\n","        print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', i)\n","    else:\n","        data_point['text'] = text_ind[:]\n","        data_point['summary'] = summ_ind[:]\n","        #print((data_point['text_len']))\n","        test_dict.append(data_point)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRPwxGySEWVe","colab_type":"code","colab":{}},"source":["data = {}\n","data['train'] = train_dict[:]\n","data['dev'] = dev_dict[:]\n","data['test'] = test_dict[:]\n","s = (data['test'][9]['summary'])[:]\n","print(s)\n","st = ''\n","for ss in s:\n","    st += ind2word[ss]\n","    st += ' '\n","print(st)\n","print(ind2word[394])\n","#print(ind2word[184],ind2word[6398],ind2word[125],ind2word[93],ind2word[249],ind2word[18],ind2word[4500],ind2word[14327],ind2word[2586],ind2word[1815],ind2word[68],ind2word[660],ind2word[14] )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8UFDSCREnoj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLb7xm8IuT1x","colab_type":"code","colab":{}},"source":["# #Padding Train and Dev Set\n","# data = {}\n","# train_data = []\n","# for obj in train_dict:\n","#     t = {}\n","#     a = obj['text'].copy()\n","#     b = obj['summary'].copy()\n","#     t['text_len'] = len(obj['text'])\n","#     while(len(a)<200):    \n","#         a.append(0)\n","#     t['text'] = a.copy()\n","#     b = [1] + b + [2] \n","#     t['summ_len'] = len(b)\n","#     while(len(b)<22):    \n","#         b.append(0)\n","#     t['summary'] = b.copy()\n","#     train_data.append(t)\n","# data['train'] =  train_data[:]   \n","\n","\n","# dev_data = []\n","# for obj in dev_dict:\n","#     t = {}\n","#     a = obj['text'].copy()\n","#     b = obj['summary'].copy()\n","#     t['text_len'] = len(obj['text'])\n","#     while(len(a)<200):    \n","#         a.append(0)\n","#     t['text'] = a.copy()\n","#     b = [1] + b + [2] \n","#     t['summ_len'] = len(b)\n","#     while(len(b)<22):    \n","#         b.append(0)\n","#     t['summary'] = b.copy()\n","#     dev_data.append(t)\n","# data['dev'] =  dev_data[:]      \n","\n","# test_data = []\n","# for obj in test_dict:\n","#     t = {}\n","#     a = obj['text'].copy()\n","#     b = obj['summary'].copy()\n","#     t['text_len'] = len(obj['text'])\n","#     while(len(a)<200):    \n","#         a.append(0)\n","#     t['text'] = a.copy()\n","#     b = [1] + b + [2] \n","#     t['summ_len'] = len(b)\n","#     while(len(b)<22):    \n","#         b.append(0)\n","#     t['summary'] = b.copy()\n","#     test_data.append(t)\n","# data['test'] =  test_data[:]      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"edrWwinX9mgm","colab_type":"code","colab":{}},"source":["trunc_norm_init_std = 1e-4\n","def init_linear_wt(linear):\n","    linear.weight.data.normal_(std=trunc_norm_init_std)\n","    if linear.bias is not None:\n","        linear.bias.data.normal_(std=trunc_norm_init_std)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNz5eWSczFZ-","colab_type":"code","colab":{}},"source":["class biEncoderLSTM(nn.Module):\n","    \n","    def __init__(self, params):\n","        super(biEncoderLSTM, self).__init__()\n","        self.hidden_size = params['hidden_size']\n","        self.emb_size = params['emb_size']\n","        self.vocab_size = params['vocab_size']\n","        self.num_layers = params['num_layers']\n","        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n","        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True, num_layers=self.num_layers, bidirectional = True)\n","        self.reduce_h = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        init_linear_wt(self.reduce_h)\n","        self.reduce_c = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        init_linear_wt(self.reduce_c)\n","\n","\n","    def forward(self, input, hidden, memory, seq_lengths):\n","        batch, seq_len = input.size()\n","        emb = self.embedding(input)\n","        packed_emb = pack_padded_sequence(emb, (seq_lengths).cpu().numpy(), batch_first = True, enforce_sorted=False)\n","        packed_output, (h, c) = self.lstm(packed_emb, (hidden, memory))\n","        output, _ = pad_packed_sequence(packed_output, batch_first = True)\n","        output1 = output[ :, :, :self.hidden_size]\n","        output2 = output[:, :, self.hidden_size:] #reverse\n","        output = output1+output2\n","        h = torch.cat(list(h), dim=1)                                   \n","        c = torch.cat(list(c), dim=1)\n","        h_reduced = F.relu(self.reduce_h(h))                       \n","        c_reduced = F.relu(self.reduce_c(c))\n","        return output, h_reduced.unsqueeze(dim=0), c_reduced.unsqueeze(dim=0)\n","\n","    def initHidden(self, batch_size):\n","        h_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n","        c_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n","        return h_init, c_init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sso-qAXHlOKX","colab_type":"code","colab":{}},"source":["class AttnDecoderLSTM(nn.Module):\n","    \n","    def __init__(self, params):\n","        super(AttnDecoderLSTM, self).__init__()\n","        self.hidden_size = params['hidden_size']\n","        self.vocab_size = params['vocab_size']\n","        self.emb_size = params['emb_size']\n","        self.num_layers = params['num_layers']\n","        self.batch_size = params['batch_size']\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first=True, num_layers = self.num_layers)\n","        self.w = nn.Linear(2*self.hidden_size, self.vocab_size)\n","        self.p_gen_linear = nn.Linear(self.hidden_size + self.emb_size, 1)\n","\n","    def forward(self, input, hidden, memory, encoder_outputs, pad_ind):\n","        pointer_gen = True\n","        batch_size, seq_len = input.size()\n","        embedded = self.embedding(input)\n","        output,(h,c) = self.lstm(embedded, (hidden, memory))\n","        attn_values = torch.bmm(output,encoder_outputs.permute(0,2,1))\n","        dec_attn_values = torch.bmm(output,output.permute(0,2,1))\n","        for b in range(batch_size):\n","            attn_values[b,:,pad_ind[b]:] = -1e10\n","        mask = torch.ones_like(dec_attn_values).cuda()\n","        mask1 = -1e10*torch.ones_like(dec_attn_values).cuda()\n","        \n","        mask = torch.tril(mask, diagonal = -1)\n","        mask1 = torch.triu(mask1)\n","        mask = mask+mask1\n","        dec_attn_values = dec_attn_values*mask.float()\n","        \n","        total_attn_values = torch.cat((attn_values,dec_attn_values),2)\n","        total_outputs = torch.cat((encoder_outputs,output),1)\n","        \n","        sftmax = nn.Softmax(dim=2)\n","        attn_dist = sftmax(attn_values)# This is attention distribution\n","        \n","        total_attn_weights = sftmax(total_attn_values) \n","        \n","        weight_attn = torch.bmm(total_attn_weights, total_outputs)\n","        final_hidden = torch.cat((output,weight_attn),2)\n","        scores = self.w(final_hidden)\n","        sftmax2 = nn.Softmax(dim=1)\n","        vocab_dist = sftmax2(scores)\n","        \n","        p_gen = None\n","        if pointer_gen:\n","            p_gen_input = torch.cat((embedded, output), 2)  # B x (hidden_dim + emb_dim)\n","            p_gen = self.p_gen_linear(p_gen_input)\n","            p_gen = torch.sigmoid(p_gen)\n","        vocab_dist_ = p_gen * vocab_dist\n","        attn_dist_ = (1 - p_gen) * attn_dist  \n","        return vocab_dist_, attn_dist_\n","\n","\n","    def initHidden(self, batch_size):\n","        h_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n","        c_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n","        return h_init, c_init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4nSu01v0L5n","colab_type":"code","colab":{}},"source":["class SuperAwesome(nn.Module):\n","    def __init__(self, params):\n","        super(SuperAwesome, self).__init__()\n","        self.bienc = biEncoderLSTM(params).cuda()\n","        self.attn_dc = AttnDecoderLSTM(params).cuda()\n","        self.vocab_size = params['vocab_size']\n","        self.bienc.embedding.weight = self.attn_dc.embedding.weight\n","    def forward(self, batch_text, batch_summary, batch_pad_len):\n","        batch_size, seq_size = batch_text.size()\n","        h_init,c_init = self.bienc.initHidden(batch_size)\n","        enc_out, h_enc, c_enc = self.bienc(batch_text,h_init,c_init, batch_pad_len)\n","        h_init, c_init = h_enc.contiguous(),c_enc.contiguous()\n","        vocab_dist_, attn_dist_ = self.attn_dc(batch_summary,h_init,c_init,enc_out, batch_pad_len)\n","        return vocab_dist_, attn_dist_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"45tYzDjDvxwh","colab_type":"code","colab":{}},"source":["from rouge import Rouge\n","\n","def calculate_rouge (hyps, refs):\n","    #print(hyps, refs)\n","    r = Rouge()\n","    scores = r.get_scores(hyps, refs, avg=True)\n","    return(scores)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipBXkxNyx3UF","colab_type":"code","colab":{}},"source":["def train_lm(dataset, params, net, fscore = False):\n","    \n","    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n","    num_examples = len(dataset)    \n","    batches = [(start, start + params['batch_size']) for start in range(0, num_examples, params['batch_size'])]\n","    \n","    for epoch in range(params['epochs']):\n","        ep_loss = 0.\n","        start_time = time.time()\n","        ref_summ = []\n","        gen_summ = []\n","        count = 0\n","        # for each batch, calculate loss and optimize model parameters            \n","        for b_idx, (start, end) in enumerate(batches):\n","            batch = dataset[start:end]\n","            batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n","            batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n","            batch_pad_len = np.zeros(len(batch))\n","            batch_summ_len = np.zeros(len(batch))\n","            i = 0\n","            for d in batch:\n","                batch_text[i] = d['text'].copy()\n","                batch_summary[i] = d['summary'].copy()\n","                batch_pad_len[i] = d['text_len']\n","                batch_summ_len[i] = d['summ_len']\n","                i += 1\n","                \n","            batch_text = torch.from_numpy(batch_text).long().cuda()\n","            batch_summary = torch.from_numpy(batch_summary).long().cuda()\n","            batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n","            batch_summ_len = torch.from_numpy(batch_summ_len).long().cuda()\n","            vocab_dist_, attn_dist_ = net(batch_text, batch_summary, batch_pad_len)\n","            _,_,l = attn_dist_.size()\n","            enc_batch_extend_vocab = batch_text.unsqueeze(1)\n","            enc_batch_extend_vocab = enc_batch_extend_vocab[:,:,:l]\n","            \n","            enc_batch_extend_vocab = enc_batch_extend_vocab.expand_as(attn_dist_)\n","            final_dist = vocab_dist_.scatter_add(2, enc_batch_extend_vocab, attn_dist_)\n","            \n","            targets = batch_summary[:, 1:].contiguous()\n","            final_dist = final_dist[:, :-1, :]\n","            gold_probs = torch.gather(final_dist, 2, targets.unsqueeze(2)).squeeze()\n","            nll = -torch.log(gold_probs +  1e-12)\n","            #remove padding loss\n","            for b in range(len(batch)):\n","                nll[b,batch_summ_len[b]-1:] = 0\n","            loss = torch.sum(nll)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            ep_loss += loss\n","            \n","            if fscore:\n","                logits = final_dist\n","                words = torch.argmax(logits, dim=2)\n","                for sent in range(words.size()[0]):\n","                    summ = ''\n","                    gold = ''\n","                    for word in range(words.size()[1]):\n","                        if ind2word[words[sent][word].item()] == 'EOS':\n","                            break\n","                        if summ :\n","                            summ += ' '\n","                        summ += ind2word[words[sent][word].item()]\n","\n","                    for word in range(1,len(batch_summary[sent])-1):\n","                        if ind2word[batch_summary[sent][word].item()] == 'EOS':\n","                            break\n","                        if gold:\n","                            gold += ' '\n","                        gold += ind2word[batch_summary[sent][word].item()]\n","\n","                    if summ == '.'*len(summ):\n","                        count += 1\n","                    if not summ or summ == '.'*len(summ):\n","                        summ = '====='\n","\n","                    gen_summ.append(summ)\n","                    ref_summ.append(gold)\n","                    \n","        idx = 5\n","        logits = final_dist[idx,:,:].unsqueeze(0)\n","        words = torch.argmax(logits, dim=2).squeeze(0)\n","        gold_summ_last = ''\n","        summ_last = ''\n","        for l in range(batch_summary[idx,1:].size()[0]):\n","            gold_summ_last += ind2word[batch_summary[idx,l].item()] + ' '\n","        for l in range(words.size()[0]):\n","            summ_last += ind2word[words[l].item()] + ' '\n","        print('########################Train######################')\n","        print(gold_summ_last)\n","        print(summ_last)\n","        \n","        if fscore:\n","            rouge_score = calculate_rouge(gen_summ, ref_summ)\n","            print('rouge', rouge_score)\n","        \n","        loss, perplex =  compute_loss(data['dev'], net, params['batch_size'], fscore = True)\n","        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev loss: %0.2f, dev perplexity: %0.2f' %\\\n","              (epoch, ep_loss, time.time()-start_time, loss, perplex))\n","        print('####################################################')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqI9kbh7Sz2Z","colab_type":"code","colab":{}},"source":["# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n","def compute_loss(dataset, net, bsz=10, fscore = False):\n","    \n","    num_examples = len(dataset)  \n","    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n","    \n","    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n","    nll = 0.\n","    gen_summ = []\n","    ref_summ = []\n","    ep_loss = 0.\n","    count = 0\n","    for b_idx, (start, end) in enumerate(batches):\n","            \n","        batch = dataset[start:end]\n","        batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n","        batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n","        batch_pad_len = np.zeros(len(batch))\n","        batch_summ_len = np.zeros(len(batch))\n","        i = 0\n","        for d in batch:\n","                batch_text[i] = d['text']\n","                batch_summary[i] = d['summary']\n","                batch_pad_len[i] = d['text_len']\n","                batch_summ_len[i] = d['summ_len']\n","                i += 1\n","        batch_text = torch.from_numpy(batch_text).long().cuda()\n","        batch_summary = torch.from_numpy(batch_summary).long().cuda()\n","        batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n","        batch_summ_len = torch.from_numpy(batch_summ_len).long().cuda()\n","        \n","        vocab_dist_, attn_dist_ = net(batch_text, batch_summary, batch_pad_len)\n","        _,_,l = attn_dist_.size()\n","        \n","        enc_batch_extend_vocab = batch_text.unsqueeze(1)\n","        enc_batch_extend_vocab = enc_batch_extend_vocab[:,:,:l]\n","        enc_batch_extend_vocab = enc_batch_extend_vocab.expand_as(attn_dist_)\n","        final_dist = vocab_dist_.scatter_add(2, enc_batch_extend_vocab, attn_dist_)\n","            \n","        targets = batch_summary[:, 1:].contiguous()\n","        final_dist = final_dist[:, :-1, :]\n","        gold_probs = torch.gather(final_dist, 2, targets.unsqueeze(2)).squeeze()\n","        nll = -torch.log(gold_probs +  1e-12)\n","        #remove padding loss\n","        for b in range(len(batch)):\n","            nll[b,batch_summ_len[b]-1:] = 0\n","        loss = torch.sum(nll)\n","        ep_loss += loss.detach()\n","        ut = torch.nonzero(batch_summary).size(0)\n","        total_unmasked_tokens += ut\n","        \n","        if fscore:\n","                logits = final_dist\n","                words = torch.argmax(logits, dim=2)\n","                for sent in range(words.size()[0]):\n","                    summ = ''\n","                    gold = ''\n","                    for word in range(words.size()[1]):\n","                        if ind2word[words[sent][word].item()] == 'EOS':\n","                            break\n","                        if summ :\n","                            summ += ' '\n","                        summ += ind2word[words[sent][word].item()]\n","\n","                    for word in range(1,len(batch_summary[sent])-1):\n","                        if ind2word[batch_summary[sent][word].item()] == 'EOS':\n","                            break\n","                        if gold:\n","                            gold += ' '\n","                        gold += ind2word[batch_summary[sent][word].item()]\n","\n","                    if summ == '.'*len(summ):\n","                        count += 1\n","                    if not summ or summ == '.'*len(summ):\n","                        summ = '====='\n","\n","                    gen_summ.append(summ)\n","                    ref_summ.append(gold)\n","        \n","    idx = 2  \n","    logits = final_dist[idx,:,:].unsqueeze(0)\n","    words = torch.argmax(logits, dim=2).squeeze(0)\n","    gold_summ_last = ''\n","    summ_last = ''\n","    for l in range(batch_summary[idx,1:].size()[0]):\n","        gold_summ_last += ind2word[batch_summary[idx,l].item()] + ' '\n","    for l in range(words.size()[0]):\n","        summ_last += ind2word[words[l].item()] + ' '        \n","    \n","    print('#############Dev###########')\n","    if fscore:\n","        rouge_score = calculate_rouge(gen_summ, ref_summ)\n","        print('rouge', rouge_score)\n","    print(gold_summ_last)\n","    print(summ_last)\n","    \n","    perplexity = torch.exp(ep_loss / total_unmasked_tokens).cpu()\n","    return ep_loss, perplexity.data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvpUpkENp8nd","colab_type":"code","colab":{}},"source":["# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n","def test_lm(dataset, net, bsz=1):\n","    num_examples = len(dataset)  \n","    batches = [(start, start + bsz) for start in\\\n","               range(0, num_examples, bsz)]\n","    nll = 0\n","    max_len = 22\n","    for b_idx, (start, end) in enumerate(batches):\n","        batch = dataset[start:end]\n","        batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n","        batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n","        batch_pad_len = np.zeros(len(batch))\n","        batch_summ_len = np.zeros(len(batch))\n","        i = 0\n","        for d in batch:\n","                batch_text[i] = d['text']\n","                batch_summary[i] = d['summary']\n","                batch_pad_len[i] = d['text_len']\n","                batch_summ_len[i] = d['summ_len']\n","                i += 1\n","        batch_text = torch.from_numpy(batch_text).long().cuda()\n","        batch_summary = torch.from_numpy(batch_summary).long().cuda()\n","        batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n","        batch_summ_len = torch.from_numpy(batch_summ_len).long().cuda()\n","        batch_summary_test = torch.ones([bsz, 1], dtype=torch.long).cuda()\n","        ans = ''\n","        count = 0\n","        for i in range(max_len):    \n","            vocab_dist_, attn_dist_ = net(batch_text, batch_summary_test, batch_pad_len)\n","            _,_,l = attn_dist_.size()\n","        \n","            enc_batch_extend_vocab = batch_text.unsqueeze(1)\n","            enc_batch_extend_vocab = enc_batch_extend_vocab[:,:,:l]\n","            enc_batch_extend_vocab = enc_batch_extend_vocab.expand_as(attn_dist_)\n","            final_dist = vocab_dist_.scatter_add(2, enc_batch_extend_vocab, attn_dist_)\n","#             print(final_dist.size())\n","            final_dist = final_dist.squeeze(0)\n","            tx = torch.argmax(final_dist,1)\n","            temp = tx[-1]\n","            temp = temp.cpu()\n","            x = torch.ones([bsz, 1], dtype=torch.long).cuda()\n","            x[0][0] = temp.item()\n","            if(ind2word[temp.item()] == \"EOS\"):\n","                break\n","            ans += ind2word[temp.item()] + ' '\n","            count += 1\n","            if(ind2word[temp.item()] == \"EOS\"):\n","                break\n","            batch_summary_test = torch.cat((batch_summary_test,x),1) \n","        #print(\"Insise test_lm: \")\n","        #print(ans)\n","    return ans, count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeztNH8Ezs5F","colab_type":"code","colab":{}},"source":["params = {}\n","params['hidden_size'] = 512\n","params['emb_size'] = 512\n","params['num_layers'] = 1\n","params['batch_size'] = 150\n","params['vocab_size'] = len(word2ind)\n","params['learning_rate'] = 0.001\n","params['epochs'] = 25\n","net = SuperAwesome(params)\n","\n","train_lm(data['train'], params, net, fscore = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7wYWyGfres8","colab_type":"code","colab":{}},"source":["test_len = []\n","#TEST TIME\n","gen_summ = []\n","ref_summ = []\n","for i in range(len(data['test'])):\n","    #print(i, end=',')\n","    y = data['test'][i]['summary']\n","    gold = ''\n","    for t in y:\n","        if(ind2word[t]=='SOS'):\n","            continue\n","        if(ind2word[t] == 'EOS'):\n","            break  \n","        gold += (ind2word[t])\n","        gold +=' '\n","    \n","    \n","    print(\"ground truth\")  \n","    print(gold)\n","    net.eval()\n","    summ, l = test_lm(data['test'][i:i+1], net) \n","    test_len.append(l)\n","    if summ == '.'*len(summ):\n","        count += 1\n","    if not summ or summ == '.'*len(summ):\n","        summ = '====='\n","    print(summ)\n","    gen_summ.append(summ)\n","    ref_summ.append(gold)\n","rouge_score = calculate_rouge(gen_summ, ref_summ)\n","print('Test rouge', rouge_score)  \n","# for n in range(num_examples):\n","#         summ = ''\n","#         for i in range(1,max_len+1):\n","#             if summ:\n","#                 summ += ' '\n","#             if int(ans[n,i]) == 2:\n","#                 break\n","#             summ += ind2word[int(ans[n,i])]\n","#         gen_summ.append(summ)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H9qHxygXCgNC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}