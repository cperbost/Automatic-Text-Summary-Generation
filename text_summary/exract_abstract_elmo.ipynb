{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6996,
     "status": "ok",
     "timestamp": 1557367393800,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "vud6621Z0oiX",
    "outputId": "cfcb3967-f79c-472f-c174-ba05aca93cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (3.4.2)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDrive\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umtkYzVauQRY"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import json, gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from google.colab import drive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1557367404089,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "DAceJTFO3yua",
    "outputId": "e56f2bb2-414f-4c75-f291-d9ad909149d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iGfx_Xv0xTj"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFUmkHfU05pX"
   },
   "outputs": [],
   "source": [
    "# Data for training set = 10k dev set = 1k\n",
    "download = drive.CreateFile({'id': '1yJB36Jj1jw3U0JwE3IwhBYDY3VI1oCGd'})\n",
    "download.GetContentFile('train.p')\n",
    "download = drive.CreateFile({'id': '1T6YxqByvjNlCSZrU7FVmdCYlxM0RHlgw'})\n",
    "download.GetContentFile('dev.p')\n",
    "download = drive.CreateFile({'id': '1vPOGkgumTOVQ4N84GBo8AAPeAgwfyMep'})\n",
    "download.GetContentFile('index2word.p')\n",
    "download = drive.CreateFile({'id': '1JRYv-RQqhX6-fdYVUmyZmP9tBOtZjp_w'})\n",
    "download.GetContentFile('word2index.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjdzHSdf0GJf"
   },
   "outputs": [],
   "source": [
    "# #Data for training set = 50k dev set = 5k\n",
    "# download = drive.CreateFile({'id': '1xq4JlZoWZ8_eO5F6VNBQvk2Mvub1Etpb'})\n",
    "# download.GetContentFile('trainL.p')\n",
    "# download = drive.CreateFile({'id': '1G3YAtOtrfv3PswNSHyGm0SIZy60n95zU'})\n",
    "# download.GetContentFile('devL.p')\n",
    "# download = drive.CreateFile({'id': '1T7rj8WokLSEEbc7-nqH48mjSnwBt4hEq'})\n",
    "# download.GetContentFile('testL.p')\n",
    "# download = drive.CreateFile({'id': '1bZXADxMyEn-yWtneA1sxDouGpuce1LRh'})\n",
    "# download.GetContentFile('index2wordL.p')\n",
    "# download = drive.CreateFile({'id': '1IMnW7Oyq4q9Ho5z87MbmVwlODjzEvY9h'})\n",
    "# download.GetContentFile('word2indexL.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pm1-CvpHZrye"
   },
   "outputs": [],
   "source": [
    "train_dict = pickle.load( open( \"train.p\", \"rb\" ) )\n",
    "dev_dict = pickle.load( open( \"dev.p\", \"rb\" ) )\n",
    "index2word = pickle.load( open( \"index2word.p\", \"rb\" ) )\n",
    "word2index = pickle.load( open( \"word2index.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1557364120872,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "KQtdZh1smt0b",
    "outputId": "1a017bac-9fc2-4a9e-b7d8-ad249343e209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205064"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index2word)\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_pacfjoFRSH"
   },
   "outputs": [],
   "source": [
    "#Extractive summary for 100 data points\n",
    "# download = drive.CreateFile({'id': '1N6K91ySpywAB9pSKVCoJZKBn84S-2UFx'})\n",
    "# download.GetContentFile('extract_train100.p')\n",
    "# train_dict = pickle.load( open( \"extract_train100.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5po_qE7Zu2_"
   },
   "outputs": [],
   "source": [
    "train_dict = train_dict[:]\n",
    "dev_dict = dev_dict[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4112,
     "status": "ok",
     "timestamp": 1557366579400,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "NdtVZtE9J6PK",
    "outputId": "3484828e-d165-4817-ed8f-5f8dff20dd68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51511\n",
      "5678\n"
     ]
    }
   ],
   "source": [
    "train_dict_text = []\n",
    "count = 0\n",
    "for i, data in enumerate(train_dict):\n",
    "    t = {}\n",
    "    text_ind = data['text'][:]\n",
    "    summ_ind = data['summary'][:]\n",
    "    text = ''\n",
    "    summ = ''\n",
    "    for txt in text_ind:\n",
    "        if text:\n",
    "            text += ' '\n",
    "        text += index2word[txt]\n",
    "    t['text'] = text\n",
    "    for s in summ_ind:\n",
    "        if summ:\n",
    "            summ += ' '\n",
    "        summ += index2word[s]\n",
    "#         if i==789:\n",
    "#             print(s, summ)\n",
    "    t['summary'] = summ\n",
    "    \n",
    "    train_dict_text.append(t)\n",
    "print(len(train_dict_text))\n",
    "\n",
    "dev_dict_text = []\n",
    "for data in dev_dict:\n",
    "    t = {}\n",
    "    text_ind = data['text'][:]\n",
    "    summ_ind = data['summary'][:]\n",
    "    text = ''\n",
    "    summ = ''\n",
    "    for txt in text_ind:\n",
    "        if text:\n",
    "            text += ' '\n",
    "        text += index2word[txt]\n",
    "    t['text'] = text\n",
    "    for s in summ_ind:\n",
    "        if summ:\n",
    "            summ += ' '\n",
    "        summ += index2word[s]\n",
    "    t['summary'] = summ\n",
    "    dev_dict_text.append(t)\n",
    "print(len(dev_dict_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1557364140110,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "_dF-Xkstxxcf",
    "outputId": "10c4ae45-90e1-4665-998c-807b006238d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advertise in the daily telegraph; product information and key facts for    advertisers .\n",
      "published: 2:27pm gmt 26 nov 2009  information and key facts about the daily telegraph .  the daily telegraph is britain 's biggest-selling quality daily newspaper , and retains the broadsheet format .  between monday and friday , it features seperate , dedicated sport and business sections .  on saturday 's , the multi-section daily telegraph includes the following:  the award-winning telegraph magazine is also included with the daily telegraph on saturdays .  big-name writers and contributors include simon heffer , boris johnson , jeff randall , brian moore , james cracknell , benedict brogan and andrew pierce .  the daily telegraph offers greater advertising impact than any compact - not only because it has bigger pages than the compacts , but also because readers spend longer with each page than any other quality daily . so , by advertising in our paper , you’ll get greater standout and more exposure .\n",
      "the%20amount%20of%20time%20it%20takes%20to%20foreclose%20on%20a%20home%20is%20increasing%2c%20meaning%20that%20delinquent%20borrowers%20can%20stay%20in%20their%20houses%20for%20months%20after%20they%20stop%20making%20payments%2c%20according%20to%20freddie%20mac\n"
     ]
    }
   ],
   "source": [
    "ind = 1\n",
    "print(train_dict_text[ind]['summary'])\n",
    "print(train_dict_text[ind]['text'])\n",
    "print(index2word[20439])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2681,
     "status": "ok",
     "timestamp": 1557364144045,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "Gy2-gfdSRQML",
    "outputId": "555b453c-6d3f-4774-f133-a0f85ff54ac6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 21:09:03.680411 139769490491264 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import tokenize\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle\n",
    "import json, gzip\n",
    "import time\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import euclidean\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpzBQZzPdR5A"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14377,
     "status": "ok",
     "timestamp": 1557364160296,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "PwxACW1NSqN1",
    "outputId": "d7d6b191-eb5b-463d-a2ca-6e9a45d803e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 21:09:19.746385 139769490491264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(module_url)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQJwjSqG8Ak_"
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(module_url)\n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13203,
     "status": "ok",
     "timestamp": 1557364179236,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "EArg_aVBkLqr",
    "outputId": "51039c69-5445-4766-93de-3b5a2cc6c3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.03819131851196289\n",
      "0.035803794860839844\n"
     ]
    }
   ],
   "source": [
    "ext_train_data = []\n",
    "ext_dev_data = []\n",
    "\n",
    "for i, data in enumerate(train_dict_text):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    start = time.time()\n",
    "    text = data['text']\n",
    "    sentences_list = tokenize.sent_tokenize(text)\n",
    "    #print((sentences_list))\n",
    "    try:\n",
    "        sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n",
    "        num_sent = sentences_embeddings.shape[0]\n",
    "        k = round(math.sqrt(num_sent))\n",
    "        clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n",
    "        cluster_centers = clustering.cluster_centers_\n",
    "        center_dist = defaultdict(list)\n",
    "        for ns in range(num_sent):\n",
    "            label = clustering.labels_[ns]\n",
    "            dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n",
    "            center_dist[label].append((dist, ns))\n",
    "\n",
    "        final = []\n",
    "        for labels in range(k):\n",
    "            try:\n",
    "                center_dist[labels].sort(key = lambda x: x[0])\n",
    "                final.append(center_dist[labels][0][1])\n",
    "            except:\n",
    "                continue\n",
    "        final.sort()\n",
    "        ex = []\n",
    "        for i in final:\n",
    "            ex.append(sentences_list[i])\n",
    "        extractive = (' ').join(ex)\n",
    "        dict_ = {}\n",
    "        dict_['text'] = extractive\n",
    "        dict_['summary'] = data['summary']\n",
    "        ext_train_data.append(dict_)\n",
    "    except:\n",
    "        print('error')\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "for i, data in enumerate(dev_dict_text):\n",
    "    start = time.time()\n",
    "    text = data['text']\n",
    "    sentences_list = tokenize.sent_tokenize(text)\n",
    "    #print((sentences_list))\n",
    "    \n",
    "    sentences_embeddings = session.run(embedded_text, feed_dict={text_input: sentences_list})\n",
    "    num_sent = sentences_embeddings.shape[0]\n",
    "    k = round(math.sqrt(num_sent))\n",
    "    clustering = KMeans(n_clusters=k).fit(sentences_embeddings)\n",
    "    cluster_centers = clustering.cluster_centers_\n",
    "    center_dist = defaultdict(list)\n",
    "    for ns in range(num_sent):\n",
    "        label = clustering.labels_[ns]\n",
    "        dist = euclidean(sentences_embeddings[ns], cluster_centers[label])\n",
    "        center_dist[label].append((dist, ns))\n",
    "            \n",
    "    final = []\n",
    "    for labels in range(k):\n",
    "        try:\n",
    "            center_dist[labels].sort(key = lambda x: x[0])\n",
    "            final.append(center_dist[labels][0][1])\n",
    "        except:\n",
    "            continue\n",
    "    final.sort()\n",
    "    ex = []\n",
    "    for i in final:\n",
    "        ex.append(sentences_list[i])\n",
    "    extractive = (' ').join(ex)\n",
    "    dict_ = {}\n",
    "    dict_['text'] = extractive\n",
    "    dict_['summary'] = data['summary']\n",
    "    ext_dev_data.append(dict_)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqSnGLzQYY2t"
   },
   "outputs": [],
   "source": [
    "ext_train_data = sorted(ext_train_data, key = lambda i: len(i['text']), reverse = True)\n",
    "# dev_dict = sorted(dev_dict, key = lambda i: len(i['text']), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1557364186380,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "zNw4b7didaoI",
    "outputId": "39950c7e-e83f-42d6-82d1-270830717ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/19/2014 at 12:00 am edt  the blue jays won the game , but this kid  a pint-sized lothario used some quick sleight of hand to convince his crush he 'd given her a foul ball at a  – but a look at the replay confirmed he 'd been planning ahead . or so it appeared . the real one he likely kept as a treasured memento of the day he went viral .\n",
      "a pint-size lothario used quick sleight of hand to convince his crush he 'd given her a foul ball\n"
     ]
    }
   ],
   "source": [
    "ind = 1\n",
    "print(ext_train_data[ind]['text'])\n",
    "print(ext_train_data[ind]['summary'])\n",
    "# print(ext_dev_data[ind]['text'])\n",
    "# print(ext_dev_data[ind]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1557364188210,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "hhHxhrkvS2NW",
    "outputId": "0c6d9935-f270-46ad-8c05-5a8f411e5ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "343\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "ind2word = {0: 'PAD', 1:'BOS', 2:'EOS', 3:'UNK'}\n",
    "word2ind = {'PAD':0, 'BOS':1, 'EOS':2, 'UNK':3}\n",
    "count = 4\n",
    "for data in ext_train_data:\n",
    "    text = data['text']\n",
    "    summ = data['summary']\n",
    "    for words in word_tokenize(text):\n",
    "        if words not in word2ind:\n",
    "            word2ind[words] = count\n",
    "            ind2word[count] = words\n",
    "            count += 1\n",
    "\n",
    "    for words in word_tokenize(summ):\n",
    "        if words not in word2ind:\n",
    "            word2ind[words] = count\n",
    "            ind2word[count] = words\n",
    "            count += 1\n",
    "print(len(word2ind))\n",
    "print(len(ind2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcjRLxxD5ltI"
   },
   "outputs": [],
   "source": [
    "# train_maxlen = 0\n",
    "# for i, data in enumerate(ext_train_data):\n",
    "#     summ = data['summary']\n",
    "#     summ_ind = [1,2]\n",
    "#     for words in word_tokenize(summ):\n",
    "#         summ_ind.append(word2ind[words])\n",
    "#     if train_maxlen < len(summ_ind):\n",
    "#         train_maxlen = len(summ_ind)\n",
    "\n",
    "# dev_maxlen = 0\n",
    "# for i, data in enumerate(ext_dev_data):\n",
    "#     summ = data['summary']\n",
    "#     summ_ind = [1,2]\n",
    "#     for words in word_tokenize(summ):\n",
    "#         if words in word2ind:\n",
    "#             summ_ind.append(word2ind[words])\n",
    "#         else:\n",
    "#             summ_ind.append(word2ind['UNK'])\n",
    "#     if dev_maxlen < len(summ_ind):\n",
    "#         dev_maxlen = len(summ_ind)\n",
    "# print(train_maxlen, dev_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuRWBuSbw4eu"
   },
   "outputs": [],
   "source": [
    "train_dict = []\n",
    "dev_dict = []\n",
    "train_maxlen = 30\n",
    "dev_maxlen = 30\n",
    "\n",
    "for i, data in enumerate(ext_train_data):\n",
    "    data_point = {}\n",
    "    text = data['text'][:]\n",
    "    summ = data['summary'][:]\n",
    "    text_ind = []\n",
    "    summ_ind = []\n",
    "    \n",
    "    for words in word_tokenize(summ):\n",
    "        summ_ind.append(word2ind[words])\n",
    "    summ_ind = [1]+summ_ind+[2]\n",
    "    while len(summ_ind)<train_maxlen:\n",
    "        summ_ind.append(0)\n",
    "    if len(summ_ind)==train_maxlen:\n",
    "        data_point['text'] = word_tokenize(text)\n",
    "        data_point['text_len'] = len(data_point['text'])\n",
    "        data_point['summary_ind'] = summ_ind[:]\n",
    "        data_point['summary'] = (['BOS']+word_tokenize(summ)+['EOS'])[:]\n",
    "        train_dict.append(data_point)\n",
    "\n",
    "    \n",
    "    \n",
    "for i, data in enumerate(ext_dev_data):\n",
    "    data_point = {}\n",
    "    text = data['text'][:]\n",
    "    summ = data['summary'][:]\n",
    "    text_ind = []\n",
    "    summ_ind = []\n",
    "    \n",
    "    for words in word_tokenize(summ):\n",
    "        if words in word2ind:\n",
    "            summ_ind.append(word2ind[words])\n",
    "        else:\n",
    "            summ_ind.append(word2ind['UNK'])\n",
    "    summ_ind = [1]+summ_ind+[2]\n",
    "    if len(summ_ind)<dev_maxlen:\n",
    "        summ_ind += [0]*(dev_maxlen-len(summ_ind))\n",
    "    \n",
    "    if len(summ_ind)==dev_maxlen:\n",
    "        data_point['text'] = word_tokenize(text)\n",
    "        data_point['text_len'] = len(data_point['text'])\n",
    "        data_point['summary_ind'] = summ_ind[:]\n",
    "        data_point['summary'] = (['BOS']+word_tokenize(summ)+['EOS'])[:]\n",
    "        dev_dict.append(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1557364193498,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "TMbeJBDZXy95",
    "outputId": "902d2094-4a8c-44ec-f1c1-40a03eb4e40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "12\n",
      "30\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dict[0]['text']))\n",
    "print(len(train_dict[0]['summary']))\n",
    "print(len(train_dict[0]['summary_ind']))\n",
    "print(train_dict[0]['text_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1557364195504,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "fX8kqMqpm0sF",
    "outputId": "3c8413be-f40a-45b3-89a4-addfbeacac62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data['train'] = train_dict\n",
    "data['dev'] = dev_dict\n",
    "print(len(data['train']))\n",
    "print(len(data['dev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYAR0NY_D135"
   },
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "from allennlp.modules.elmo import Elmo\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJpl3W4ae9l3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXjDxOKTy0Ck"
   },
   "outputs": [],
   "source": [
    "# class EncoderLSTM(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params):\n",
    "#         super(EncoderLSTM, self).__init__()\n",
    "#         self.hidden_size = params['hidden_size']\n",
    "#         self.emb_size = params['emb_size']\n",
    "#         self.vocab_size = params['vocab_size']\n",
    "#         self.num_layers = params['num_layers']\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "#         self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True, num_layers=self.num_layers)\n",
    "\n",
    "#     def forward(self, input, hidden, memory, seq_lengths):\n",
    "#         batch, seq_len = input.size()\n",
    "#         character_ids = batch_to_ids(input).cuda()\n",
    "#         emb = self.embedding(input)\n",
    "#         packed_emb = pack_padded_sequence(emb, (seq_lengths).cpu().numpy(), batch_first = True)\n",
    "#         packed_output, (h, c) = self.lstm(packed_emb, (hidden, memory))\n",
    "#         output, _ = pad_packed_sequence(packed_output, batch_first = True)\n",
    "#         return output, h, c\n",
    "\n",
    "#     def initHidden(self, batch_size):\n",
    "#         h_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "#         c_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "#         return h_init, c_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNz5eWSczFZ-"
   },
   "outputs": [],
   "source": [
    "class biEncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(biEncoderLSTM, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.emb_size = params['emb_size']\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True, num_layers=self.num_layers, bidirectional = True)\n",
    "        self.elmo = Elmo(options_file, weight_file, 1, requires_grad=True, dropout=0)\n",
    "        \n",
    "    def forward(self, input, hidden, memory, seq_lengths):\n",
    "        batch = len(input)\n",
    "        character_ids = batch_to_ids(input).cuda()\n",
    "        emb = (self.elmo(character_ids)['elmo_representations'][0]).cuda()\n",
    "        output, (h, c) = self.lstm(emb, (hidden, memory))\n",
    "        output1 = output[ :, :, :self.hidden_size]\n",
    "        output2 = output[:, :, self.hidden_size:] #reverse\n",
    "        output = output1+output2\n",
    "        h = h.view(self.num_layers, 2, batch, self.hidden_size)\n",
    "        c = c.view(self.num_layers, 2, batch, self.hidden_size)\n",
    "        h = h[:,0,:,:]\n",
    "        c = c[:,0,:,:]\n",
    "        return output, h, c\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        h_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n",
    "        c_init = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n",
    "        return h_init, c_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NmDay6PEmvN"
   },
   "outputs": [],
   "source": [
    "# class DecoderLSTM(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params):\n",
    "#         super(DecoderLSTM, self).__init__()\n",
    "#         self.hidden_size = params['hidden_size']\n",
    "#         self.vocab_size = params['vocab_size']\n",
    "#         self.emb_size = params['emb_size']\n",
    "#         self.num_layers = params['num_layers']\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "#         self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first = True)\n",
    "#         self.w = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "#     def forward(self, input, hidden, memory):\n",
    "#         batch_size, seq_len = input.size()\n",
    "#         emb = self.embedding(input)\n",
    "#         output, (h, c) = self.lstm(emb, (hidden, memory))\n",
    "#         logits = self.w(output)\n",
    "#         return logits, h, c\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         h_init = torch.randn(self.num_layers, batch_size, self.d_hid).cuda()\n",
    "#         c_init = torch.randn(self.num_layers, batch_size, self.d_hid).cuda()\n",
    "#         return h_init, c_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icvY8qLMznrh"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = params['hidden_size']\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.emb_size = params['emb_size']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.batch_size = params['batch_size']\n",
    "        #self.dropout_p = params['dropout_p']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        #self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, batch_first=True, num_layers = self.num_layers)\n",
    "        self.w = nn.Linear(2*self.hidden_size, self.vocab_size)\n",
    "        self.elmo = Elmo(options_file, weight_file, 1, requires_grad=True, dropout=0)\n",
    "\n",
    "    def forward(self, input, hidden, memory, encoder_outputs, pad_ind):\n",
    "        batch_size = len(input)\n",
    "        character_ids = batch_to_ids(input).cuda()\n",
    "        #embedded = self.embedding(input)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        embedded = (self.elmo(character_ids)['elmo_representations'][0]).cuda()        \n",
    "        output,(h,c) = self.lstm(embedded, (hidden, memory))\n",
    "        \n",
    "        attn_values = torch.bmm(output,encoder_outputs.permute(0,2,1))\n",
    "        dec_attn_values = torch.bmm(output,output.permute(0,2,1))\n",
    "    \n",
    "        for b in range(batch_size):\n",
    "            attn_values[b,:,pad_ind[b]:] = -1e10\n",
    "        \n",
    "        mask = torch.ones_like(dec_attn_values).cuda()\n",
    "        mask1 = -1e10*torch.ones_like(dec_attn_values).cuda()\n",
    "        mask = torch.tril(mask, diagonal = -1)\n",
    "        mask1 = torch.triu(mask1)\n",
    "        mask = mask+mask1\n",
    "        dec_attn_values = dec_attn_values*mask.float()\n",
    "        \n",
    "        total_attn_values = torch.cat((attn_values,dec_attn_values),2)\n",
    "        total_outputs = torch.cat((encoder_outputs,output),1)\n",
    "        \n",
    "        sftmax = nn.Softmax(dim=2)\n",
    "        total_attn_weights = sftmax(total_attn_values)\n",
    "        weight_attn = torch.bmm(total_attn_weights, total_outputs)\n",
    "        final_hidden = torch.cat((output,weight_attn),2)\n",
    "        scores = self.w(final_hidden)\n",
    "        return scores, h, c\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        h_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "        c_init = torch.randn(self.num_layers, batch_size, self.hidden_size).cuda()\n",
    "        return h_init, c_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47xPxFOSxUr_"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge (hyps, refs):\n",
    "    #print(hyps, refs)\n",
    "    r = Rouge()\n",
    "    scores = r.get_scores(hyps, refs, avg=True)\n",
    "    return(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ii83OBfz8vf"
   },
   "outputs": [],
   "source": [
    "def train_lm(dataset, params, net, fscore = False):\n",
    "    \n",
    "    #print(dataset[9920]['summary'])\n",
    "    # since the first index corresponds to the PAD token, we just ignore it\n",
    "    # when computing the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    sftmax = nn.Softmax(dim=2)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    num_examples = len(dataset)    \n",
    "    batches = [(start, start + params['batch_size']) for start in range(0, num_examples, params['batch_size'])]\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        #random.shuffle(batches)\n",
    "        ref_summ = []\n",
    "        gen_summ = []\n",
    "        count = 0\n",
    "        # for each batch, calculate loss and optimize model parameters            \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            batch = dataset[start:end]\n",
    "            \n",
    "            batch_text = []\n",
    "            batch_summary = []\n",
    "            batch_pad_len = []\n",
    "#             batch_summ_indexes = []\n",
    "            batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n",
    "            i = 0\n",
    "            for d in batch:\n",
    "                \n",
    "                batch_text.append(d['text'])\n",
    "                batch_summary.append(d['summary'])\n",
    "                batch_pad_len.append(d['text_len'])\n",
    "#                 batch_summ_indexes.append(d['summary_ind'])\n",
    "                batch_summ_indexes[i] = d['summary_ind']\n",
    "                #print(len(d['summary_ind']), len(d['summary']))\n",
    "                \n",
    "                i += 1\n",
    "            \n",
    "            \n",
    "            batch_summ_indexes = torch.from_numpy(batch_summ_indexes).long().cuda()\n",
    "#             batch_summ_indexes = torch.tensor(batch_summ_indexes).long.cuda()\n",
    "            pred = net(batch_text, batch_summary, batch_pad_len)\n",
    "            sz = pred.size()[1]\n",
    "            preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch_summ_indexes[:, 1:sz].contiguous().view(-1)\n",
    "            #print(pred.size(),preds.size(),targets.size(),batch_summ_indexes.size())\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss\n",
    "            \n",
    "            if fscore:\n",
    "                logits = sftmax(pred)\n",
    "                words = torch.argmax(logits, dim=2)\n",
    "                for sent in range(words.size()[0]):\n",
    "                    summ = ''\n",
    "                    gold = ''\n",
    "                    for word in range(words.size()[1]):\n",
    "                        if ind2word[words[sent][word].item()] == 'EOS':\n",
    "                            break\n",
    "                        if summ :\n",
    "                            summ += ' '\n",
    "                        summ += ind2word[words[sent][word].item()]\n",
    "\n",
    "                    for word in range(1,len(batch_summary[sent])-1):\n",
    "                        if ind2word[batch_summ_indexes[sent][word].item()] == 'EOS':\n",
    "                            break\n",
    "                        if gold:\n",
    "                            gold += ' '\n",
    "                        gold += ind2word[batch_summ_indexes[sent][word].item()]\n",
    "\n",
    "                    if summ == '.'*len(summ):\n",
    "                        count += 1\n",
    "                    if not summ or summ == '.'*len(summ):\n",
    "                        summ = '====='\n",
    "                    if not gold or gold=='.'*len(gold):\n",
    "                        continue\n",
    "\n",
    "                    gen_summ.append(summ)\n",
    "                    ref_summ.append(gold)\n",
    "        print(start)   \n",
    "        ind = 0\n",
    "        scores = pred[ind,:,:].unsqueeze(0)\n",
    "        logits = sftmax(scores)\n",
    "        words = torch.argmax(logits, dim=2).squeeze(0)\n",
    "        gold_summ_last = ''\n",
    "        summ_last = ''\n",
    "        for l in range(batch_summ_indexes[ind,1:].size()[0]):\n",
    "            gold_summ_last += ind2word[batch_summ_indexes[ind,l].item()] + ' '\n",
    "        for l in range(words.size()[0]):\n",
    "            summ_last += ind2word[words[l].item()] + ' '\n",
    "        print('########################Train######################')\n",
    "        print(gold_summ_last)\n",
    "        print(summ_last)\n",
    "        \n",
    "        #print(len(gen_summ), len(ref_summ))\n",
    "        if fscore:\n",
    "            rouge_score = calculate_rouge(gen_summ, ref_summ)\n",
    "            print('rouge', rouge_score)\n",
    "        \n",
    "#         print('epoch: %d, loss: %0.2f, time: %0.2f sec' %\\\n",
    "#                 (epoch, ep_loss, time.time()-start_time))\n",
    "        loss, perplex =  compute_loss(data['dev'], net, params['batch_size'], fscore = True)\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev loss: %0.2f, dev perplexity: %0.2f' %\\\n",
    "              (epoch, ep_loss, time.time()-start_time, loss, perplex))\n",
    "        print('####################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rwJWkwYz-LT"
   },
   "outputs": [],
   "source": [
    "def compute_loss(dataset, net, bsz=10, fscore = False):\n",
    "    \n",
    "    sftmax= nn.Softmax(dim=2)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    num_examples = len(dataset)  \n",
    "    # we'll still use batches b/c we can't fit the whole\n",
    "    # validation set into GPU memory\n",
    "    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "    \n",
    "    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n",
    "    nll = 0.\n",
    "    gen_summ = []\n",
    "    ref_summ = []\n",
    "    count = 0\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "#         print(start, end)\n",
    "        batch = dataset[start:end]\n",
    "    \n",
    "        batch_text = []\n",
    "        batch_summary = []\n",
    "        batch_pad_len = []\n",
    "        batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n",
    "        i = 0\n",
    "        for d in batch:\n",
    "            batch_text.append(d['text'])\n",
    "            batch_summary.append(d['summary'])\n",
    "            batch_pad_len.append(d['text_len'])\n",
    "            batch_summ_indexes[i] = d['summary_ind']\n",
    "            i += 1\n",
    "                \n",
    "        #batch_text = torch.from_numpy(np.array(batch_text)).long().cuda()\n",
    "        #batch_summary = torch.from_numpy(np.array(batch_summary)).long().cuda()\n",
    "        batch_pad_len = torch.from_numpy(np.array(batch_pad_len)).long().cuda()\n",
    "        batch_summ_indexes = torch.from_numpy(batch_summ_indexes).long().cuda()\n",
    "#        ut = torch.nonzero(batch_text).size(0)\n",
    "        pred = net(batch_text, batch_summary, batch_pad_len)\n",
    "        preds = pred[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        sz = pred.size()[1]\n",
    "        targets = batch_summ_indexes[:, 1:sz].contiguous().view(-1)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    " #       total_unmasked_tokens += ut\n",
    "        \n",
    "        if fscore:\n",
    "            #print('pred', pred.size())\n",
    "            logits = sftmax(pred)\n",
    "            words = torch.argmax(logits, dim=2)\n",
    "            #print('words', words.size())\n",
    "            for sent in range(words.size()[0]):\n",
    "                summ = ''\n",
    "                gold = ''\n",
    "                for word in range(words.size()[1]):\n",
    "                    if ind2word[words[sent][word].item()] == 'EOS':\n",
    "                        break\n",
    "                    if summ :\n",
    "                        summ += ' '\n",
    "                    summ += ind2word[words[sent][word].item()]\n",
    "\n",
    "                for word in range(1,len(batch_summ_indexes[sent])-1):\n",
    "                    if ind2word[batch_summ_indexes[sent][word].item()] == 'EOS':\n",
    "                        break\n",
    "                    if gold:\n",
    "                        gold += ' '\n",
    "                    gold += ind2word[batch_summ_indexes[sent][word].item()]\n",
    "\n",
    "                if summ == '.'*len(summ):\n",
    "                    count += 1\n",
    "                if not summ or summ == '.'*len(summ):\n",
    "                    summ = '====='\n",
    "                if not gold or gold=='.'*len(gold):\n",
    "                    continue\n",
    "\n",
    "                gen_summ.append(summ)\n",
    "                ref_summ.append(gold)\n",
    "        \n",
    "    ind = 0\n",
    "    scores = pred[ind,:,:].unsqueeze(0)\n",
    "    logits = sftmax(scores)\n",
    "    words = torch.argmax(logits, dim=2).squeeze(0)\n",
    "    gold_summ = ''\n",
    "    summ = ''\n",
    "    for l in range(batch_summ_indexes[ind,1:].size()[0]):\n",
    "        gold_summ += ind2word[batch_summ_indexes[ind,l].item()] + ' '\n",
    "    for l in range(words.size()[0]):\n",
    "        summ += ind2word[words[l].item()] + ' '\n",
    "            \n",
    "    \n",
    "    print('#############Dev###########')\n",
    "    if fscore:\n",
    "        rouge_score = calculate_rouge(gen_summ, ref_summ)\n",
    "        print('rouge', rouge_score)\n",
    "    print(gold_summ)\n",
    "    print(summ)\n",
    "    \n",
    "    perplexity = torch.exp(nll).cpu() #torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return nll, perplexity.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_jUbftcNaJS"
   },
   "outputs": [],
   "source": [
    "# # function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "# def test_lm(dataset, net, bsz=1, beam_search = False, max_len = 22):\n",
    "    \n",
    "#     sftmax = nn.Softmax(dim=1)\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "#     num_examples = len(dataset)  \n",
    "#     # we'll still use batches b/c we can't fit the whole\n",
    "#     # validation set into GPU memory\n",
    "#     batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "#     nll = 0\n",
    "#     ans = []\n",
    "#     for b_idx, (start, end) in enumerate(batches):\n",
    "            \n",
    "#         batch = dataset[start:end]\n",
    "#         batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n",
    "#         batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n",
    "#         batch_pad_len = np.zeros([len(batch)])\n",
    "#         i = 0\n",
    "        \n",
    "#         for d in batch:\n",
    "#             batch_text[i] = d['text']\n",
    "#             batch_summary[i] = d['summary']\n",
    "#             batch_pad_len[i] = d['text_len']\n",
    "#             i += 1\n",
    "            \n",
    "#         batch_text = torch.from_numpy(batch_text).long().cuda()\n",
    "#         batch_summary = torch.from_numpy(batch_summary).long().cuda()\n",
    "#         batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n",
    "        \n",
    "#         batch_summary_gen = torch.ones([bsz, 1], dtype=torch.long).cuda()\n",
    "        \n",
    "#         for i in range(max_len):    \n",
    "#             preds = net(batch_text, batch_summary_gen, batch_pad_len)[:,-1,:]\n",
    "#             distribution = sftmax(preds)\n",
    "#             ind = torch.argmax(distribution,1).cpu()\n",
    "#             if(ind2word[ans[-1]] == \"EOS\"):\n",
    "#                 break\n",
    "# #             batch_summary_test = torch.cat((batch_summary_test,x),1) \n",
    "# #         print(\"Inside test_lm: \")\n",
    "# #         print(*ans, sep = ' ')\n",
    "# # #         targets = batch_summary[:, 1:].contiguous().view(-1)\n",
    "# # #         preds = preds[:-1, :].contiguous().view(-1, net.vocab_size)\n",
    "# # #         loss = criterion(preds, targets)\n",
    "# # #         nll += loss.detach()\n",
    "\n",
    "#     return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyLS40d_0Iqw"
   },
   "outputs": [],
   "source": [
    "# # function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "# def test_lm_bms(dataset, net, bsz=1):\n",
    "    \n",
    "#     net1 = net\n",
    "#     net2 = net\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "#     num_examples = len(dataset)  \n",
    "    \n",
    "#     batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "    \n",
    "#     nll = 0\n",
    "#     max_len = 22\n",
    "#     for b_idx, (start, end) in enumerate(batches):\n",
    "            \n",
    "#         batch = dataset[start:end]\n",
    "#         batch_text = np.zeros([len(batch), len(batch[0]['text'])])\n",
    "#         batch_summary = np.zeros([len(batch), len(batch[0]['summary'])])\n",
    "#         batch_pad_len = np.zeros([len(batch)])\n",
    "#         i = 0\n",
    "#         ans = []\n",
    "#         for d in batch:\n",
    "#             batch_text[i] = d['text']\n",
    "#             batch_summary[i] = d['summary']\n",
    "#             batch_pad_len[i] = d['text_len']\n",
    "#             i += 1\n",
    "#         batch_text = torch.from_numpy(batch_text).long().cuda()\n",
    "#         gold_batch_summary = torch.from_numpy(batch_summary).long().cuda()\n",
    "#         batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n",
    "        \n",
    "#         dec_batch_summary1 = torch.ones([bsz, 1], dtype=torch.long).cuda()\n",
    "#         dec_batch_summary2 = torch.ones([bsz, 1], dtype=torch.long).cuda()\n",
    "#         for i in range(max_len):    \n",
    "            \n",
    "#             if dec_batch_summary1[0][-1]==2 and dec_batch_summary2[0][-1]==2:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 if dec_batch_summary1[0][-1]!=2:\n",
    "#                     preds1 = net1(batch_text, dec_batch_summary1, batch_pad_len)\n",
    "#                     preds1 = preds1.contiguous().view(-1, net.vocab_size)\n",
    "#                     sftmax = nn.Softmax(dim=1)\n",
    "#                     distribution1 = sftmax(preds1)\n",
    "#                     prob1, top_words1 = torch.topk(distribution1[-1],k=2,dim=0)\n",
    "#                     #print('1',prob1, top_words1)\n",
    "\n",
    "#                 else:\n",
    "#                     prob1 = -1e40*torch.ones_like(prob1)\n",
    "\n",
    "#                 if dec_batch_summary2[0][-1]!=2:\n",
    "#                     preds2 = net2(batch_text, dec_batch_summary2, batch_pad_len)\n",
    "#                     preds2 = preds2.contiguous().view(-1, net.vocab_size)\n",
    "#                     sftmax = nn.Softmax(dim=1)\n",
    "#                     distribution2 = sftmax(preds2)\n",
    "#                     prob2, top_words2 = torch.topk(distribution2[-1],k=2,dim=0)\n",
    "#                     #print('2',prob2, top_words2)\n",
    "\n",
    "#                 else:\n",
    "#                     prob2 = -1e40*torch.ones_like(prob2)\n",
    "\n",
    "#                 prob_cat = torch.cat((prob1, prob2))\n",
    "#                 top_words_cat = torch.cat((top_words1, top_words2))\n",
    "#                 final_top_words = torch.topk(prob_cat,k=2,dim=0)[1]\n",
    "#                 #print(prob_cat, final_top_words)\n",
    "#                 #print(dec_batch_summary1)\n",
    "\n",
    "#                 if final_top_words[0] in [0,1]:\n",
    "#                     dec_batch_summary1 = torch.cat((dec_batch_summary1,top_words1[final_top_words[0]].view(-1,1)),1) \n",
    "#                 elif final_top_words[0] in [2,3]:\n",
    "#                     dec_batch_summary1 = torch.cat((dec_batch_summary2,top_words2[final_top_words[0]-2].view(-1,1)),1)\n",
    "#                 if final_top_words[1] in [0,1]:\n",
    "#                     dec_batch_summary2 = torch.cat((dec_batch_summary1,top_words1[final_top_words[1]].view(-1,1)),1) \n",
    "#                 elif final_top_words[0] in [2,3]:\n",
    "#                     dec_batch_summary2 = torch.cat((dec_batch_summary2,top_words2[final_top_words[1]-2].view(-1,1)),1)\n",
    "            \n",
    "#             #print(dec_batch_summary1)\n",
    "#             #print(ind2word[temp.item()])\n",
    "#             #if(ind2word[temp.item()] == \"EOS\"):\n",
    "#             #    break\n",
    "#            # batch_summary_test = torch.cat((batch_summary_test,x),1) \n",
    "# #         targets = batch_summary[:, 1:].contiguous().view(-1)\n",
    "# #         preds = preds[:-1, :].contiguous().view(-1, net.vocab_size)\n",
    "# #         loss = criterion(preds, targets)\n",
    "# #         nll += loss.detach()\n",
    "    \n",
    "#         summ_str1 = ''\n",
    "#         summ_str2 = ''\n",
    "#         for l in range(dec_batch_summary1.size()[1]):\n",
    "#             summ_str1 += ind2word[dec_batch_summary1[0][l].item()]+' '\n",
    "#         for l in range(dec_batch_summary2.size()[1]):\n",
    "#             summ_str2 += ind2word[dec_batch_summary2[0][l].item()]+' '\n",
    "    \n",
    "#     return nll, summ_str1,summ_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOIXr5QYlu35"
   },
   "outputs": [],
   "source": [
    "# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "def test_lm(dataset, net, bsz=1, beam_search = False, max_len = 30):\n",
    "    \n",
    "    sftmax = nn.Softmax(dim=1)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples = len(dataset)  \n",
    "    # we'll still use batches b/c we can't fit the whole\n",
    "    # validation set into GPU memory\n",
    "    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "    nll = 0\n",
    "    ans = []\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "            \n",
    "        batch = dataset[start:end]\n",
    "        batch_text = []\n",
    "        batch_summary = []\n",
    "        batch_pad_len = []\n",
    "        batch_summ_indexes = np.zeros([len(batch), len(batch[0]['summary_ind'])])\n",
    "        i = 0\n",
    "        \n",
    "        for d in batch:\n",
    "                batch_text.append(d['text'])\n",
    "                batch_summary.append(d['summary'])\n",
    "                batch_pad_len.append(d['text_len'])\n",
    "                batch_summ_indexes[i] = d['summary_ind']\n",
    "                i += 1\n",
    "                \n",
    "#         batch_text = torch.from_numpy(batch_text).long().cuda()\n",
    "#         batch_summary = torch.from_numpy(batch_summary).long().cuda()\n",
    "#         batch_pad_len = torch.from_numpy(batch_pad_len).long().cuda()\n",
    "        \n",
    "#         batch_summary_gen = torch.ones([bsz, 1], dtype=torch.long).cuda()\n",
    "#         bos = ['BOS']\n",
    "        batch_summary_gen = [['BOS'] for i in range(bsz)]\n",
    "        \n",
    "        for i in range(max_len):    \n",
    "            preds = net(batch_text, batch_summary_gen, batch_pad_len)[:,-1,:]\n",
    "            distribution = sftmax(preds)\n",
    "            ind = torch.argmax(distribution,1)\n",
    "            for j in range(bsz):\n",
    "              batch_summary_gen[j].append(ind2word[ind[j].item()])\n",
    "        \n",
    "        ans+=batch_summary_gen.copy()\n",
    "    \n",
    "    gen_summ = []\n",
    "    gold_summ = []\n",
    "    \n",
    "    for n in range(num_examples):\n",
    "        summ = ''\n",
    "#         summ = (' ').join(ans[n])\n",
    "        for k in ans[n][1:]:\n",
    "          if k == 'EOS':\n",
    "            break\n",
    "          summ += k + ' '  \n",
    "        gen_summ.append(summ)\n",
    "        \n",
    "    for n in range(num_examples):\n",
    "        summ = ''\n",
    "#         summ = (' ').join(dataset[n]['summary'])\n",
    "        for k in dataset[n]['summary'][1:]:\n",
    "          if k == 'EOS':\n",
    "            break\n",
    "          summ += k + ' '\n",
    "        gold_summ.append(summ)\n",
    "    print((gold_summ))\n",
    "    print((gen_summ))\n",
    "    rouge_score = calculate_rouge(gen_summ, gold_summ)\n",
    "    print('rouge', rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4nSu01v0L5n"
   },
   "outputs": [],
   "source": [
    "class SuperAwesome(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(SuperAwesome, self).__init__()\n",
    "        #self.enc = EncoderLSTM(params).cuda()\n",
    "        self.bienc = biEncoderLSTM(params).cuda()\n",
    "        #self.dc = DecoderLSTM(params).cuda()\n",
    "        self.attn_dc = AttnDecoderLSTM(params).cuda()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "    def forward(self, batch_text, batch_summary, batch_pad_len):\n",
    "        batch_size = len(batch_text)\n",
    "        \n",
    "#         h_init,c_init = self.enc.initHidden(batch_size)\n",
    "#         enc_output,h_enc,c_enc = self.enc(batch_text,h_init,c_init, batch_pad_len)\n",
    "        \n",
    "        h_init,c_init = self.bienc.initHidden(batch_size)\n",
    "        enc_output,h_enc,c_enc = self.bienc(batch_text,h_init,c_init, batch_pad_len)\n",
    "        #h_init,c_init = self.attn_dc.initHidden(batch_size)\n",
    "        h_init, c_init = h_enc.contiguous(),c_enc.contiguous()\n",
    "        attn_pre,h,c = self.attn_dc(batch_summary,h_init,c_init,enc_output, batch_pad_len)\n",
    "#         attn_pre, h, c = self.dc(batch_summary, h_enc, c_enc)\n",
    "        return attn_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5647
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 70858,
     "status": "ok",
     "timestamp": 1557364669030,
     "user": {
      "displayName": "Shyla Gangwar",
      "photoUrl": "",
      "userId": "17693230568302739601"
     },
     "user_tz": 240
    },
    "id": "jeztNH8Ezs5F",
    "outputId": "f5443942-53d7-45f6-f69d-d8419ae4780e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "marissa pennsylvania portion former nbc at at at at at at at at at planning by by by by by by by \n",
      "rouge {'rouge-1': {'f': 0.02687746887733764, 'p': 0.030202020202020202, 'r': 0.024358974358974356}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.026151330768507335, 'p': 0.030202020202020202, 'r': 0.024358974358974356}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.04523809467715421, 'p': 0.15, 'r': 0.02909090909090909}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.030963670437366475, 'p': 0.15, 'r': 0.02909090909090909}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 0, loss: 5.86, time: 1.85 sec, dev loss: 5.72, dev perplexity: 305.67\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "co the co EOS EOS EOS EOS EOS . EOS EOS EOS EOS EOS EOS co co co co co co co \n",
      "rouge {'rouge-1': {'f': 0.05243107687360558, 'p': 0.25, 'r': 0.02991452991452991}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.030468243189883136, 'p': 0.25, 'r': 0.02991452991452991}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.11857142757597508, 'p': 0.4, 'r': 0.07075757575757577}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.07299861719218408, 'p': 0.4, 'r': 0.07075757575757577}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the the the the EOS EOS . EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 1, loss: 5.08, time: 1.58 sec, dev loss: 7.53, dev perplexity: 1859.50\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the the the the EOS . . . . EOS EOS EOS EOS EOS EOS EOS EOS the the the the the \n",
      "rouge {'rouge-1': {'f': 0.05934065879132956, 'p': 0.4, 'r': 0.03205128205128205}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.03224130880852754, 'p': 0.4, 'r': 0.03205128205128205}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.1119047601038549, 'p': 0.14166666666666666, 'r': 0.09837662337662337}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.10296899167862081, 'p': 0.14166666666666666, 'r': 0.09837662337662337}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "our 's and and and and and and and and a and and a and and and and and and a and \n",
      "epoch: 2, loss: 4.29, time: 1.59 sec, dev loss: 7.86, dev perplexity: 2592.13\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "our a a a a a and a a a a 's and and and and and and and and and and \n",
      "rouge {'rouge-1': {'f': 0.1391902189212873, 'p': 0.29166666666666663, 'r': 0.0943070818070818}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.09330525858295009, 'p': 0.26666666666666666, 'r': 0.0866147741147741}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.03999999952000001, 'p': 0.05, 'r': 0.03333333333333333}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0371428571428493, 'p': 0.05, 'r': 0.03333333333333333}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "take signees that a that that that that that that EOS that that that that EOS EOS that EOS EOS EOS EOS \n",
      "epoch: 3, loss: 4.48, time: 1.60 sec, dev loss: 8.08, dev perplexity: 3236.29\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "take media to to to to to for for for to to a a a a a a EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.08554536823505753, 'p': 0.25, 'r': 0.05253357753357753}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.05472358364934329, 'p': 0.25, 'r': 0.05253357753357753}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise signees EOS EOS EOS EOS for EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 4, loss: 4.23, time: 1.59 sec, dev loss: 9.26, dev perplexity: 10529.99\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise signees will for EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.07455128052933954, 'p': 0.21666666666666665, 'r': 0.046581196581196575}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.04889887980036173, 'p': 0.21666666666666665, 'r': 0.046581196581196575}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.05555555461111113, 'p': 0.06666666666666667, 'r': 0.05}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.05185185185180925, 'p': 0.06666666666666667, 'r': 0.05}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "radcliffe signees EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 5, loss: 4.07, time: 1.58 sec, dev loss: 9.09, dev perplexity: 8891.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "radcliffe vc effort vc a EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.12061771368050252, 'p': 0.3, 'r': 0.07788461538461539}, 'rouge-2': {'f': 0.040952380083038566, 'p': 0.125, 'r': 0.02638888888888889}, 'rouge-l': {'f': 0.08262527586839442, 'p': 0.3, 'r': 0.07788461538461539}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.09007936333964005, 'p': 0.13333333333333333, 'r': 0.07075757575757577}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.07793236836915282, 'p': 0.13333333333333333, 'r': 0.07075757575757577}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "raytheon vc the the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 6, loss: 3.85, time: 1.59 sec, dev loss: 8.77, dev perplexity: 6416.42\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "raytheon vc effort effort effort EOS EOS EOS EOS the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.13261904550487674, 'p': 0.36666666666666664, 'r': 0.08205128205128205}, 'rouge-2': {'f': 0.04609943739379345, 'p': 0.12333333333333334, 'r': 0.02949346405228758}, 'rouge-l': {'f': 0.08567043423047153, 'p': 0.36666666666666664, 'r': 0.08205128205128205}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.10586080450294517, 'p': 0.25, 'r': 0.07075757575757577}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.075968305169545, 'p': 0.25, 'r': 0.07075757575757577}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the the . . . EOS . EOS . . . EOS EOS . . EOS . . EOS . . EOS \n",
      "epoch: 7, loss: 3.74, time: 1.59 sec, dev loss: 9.98, dev perplexity: 21523.80\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the co the the the the the the the the EOS the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.09476190348344672, 'p': 0.4, 'r': 0.05427350427350427}, 'rouge-2': {'f': 0.00833333295833335, 'p': 0.016666666666666666, 'r': 0.005555555555555555}, 'rouge-l': {'f': 0.05529792974597268, 'p': 0.4, 'r': 0.05427350427350427}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.11499999887444448, 'p': 0.35, 'r': 0.07075757575757577}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.07380506880509315, 'p': 0.35, 'r': 0.07075757575757577}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the the EOS the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 8, loss: 3.69, time: 1.60 sec, dev loss: 9.89, dev perplexity: 19800.25\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the vc the the the the the the the the EOS the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.12293373151090378, 'p': 0.55, 'r': 0.07094017094017094}, 'rouge-2': {'f': 0.00833333295833335, 'p': 0.016666666666666666, 'r': 0.005555555555555555}, 'rouge-l': {'f': 0.07256897687030295, 'p': 0.55, 'r': 0.07094017094017094}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.09395604236214303, 'p': 0.16666666666666666, 'r': 0.07075757575757577}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.07703341625160295, 'p': 0.16666666666666666, 'r': 0.07075757575757577}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees the the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 9, loss: 3.45, time: 1.60 sec, dev loss: 10.24, dev perplexity: 28013.51\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the vc performs a the on the the the the EOS the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.1695496870721705, 'p': 0.38999999999999996, 'r': 0.11057692307692309}, 'rouge-2': {'f': 0.027314813995370398, 'p': 0.04722222222222222, 'r': 0.01944444444444444}, 'rouge-l': {'f': 0.11735918499332387, 'p': 0.38999999999999996, 'r': 0.11057692307692309}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.136753243661272, 'p': 0.17916666666666664, 'r': 0.13087662337662337}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.11846828961926006, 'p': 0.17916666666666664, 'r': 0.13087662337662337}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees has EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 10, loss: 3.26, time: 1.62 sec, dev loss: 10.82, dev perplexity: 50076.32\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "take vc effort will focus on the EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.24476136261448925, 'p': 0.43476190476190474, 'r': 0.17938034188034185}, 'rouge-2': {'f': 0.09400560085587556, 'p': 0.12333333333333334, 'r': 0.07777777777777778}, 'rouge-l': {'f': 0.18576876014807922, 'p': 0.4097619047619048, 'r': 0.1716880341880342}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.10548340271416919, 'p': 0.13499999999999998, 'r': 0.09436868686868685}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.09699614948365894, 'p': 0.13499999999999998, 'r': 0.09436868686868685}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees has EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "epoch: 11, loss: 3.05, time: 1.59 sec, dev loss: 11.29, dev perplexity: 80209.45\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the vc effort will focus on capital-efficient EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.3070224051637846, 'p': 0.48238095238095235, 'r': 0.23394383394383395}, 'rouge-2': {'f': 0.09642857007158805, 'p': 0.12261904761904763, 'r': 0.08194444444444446}, 'rouge-l': {'f': 0.2551808383351119, 'p': 0.48238095238095235, 'r': 0.23394383394383395}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.087460315190572, 'p': 0.11166666666666666, 'r': 0.08186868686868685}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0789162962425114, 'p': 0.11166666666666666, 'r': 0.08186868686868685}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the automaker daily EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS for EOS \n",
      "epoch: 12, loss: 2.92, time: 1.59 sec, dev loss: 11.97, dev perplexity: 157299.22\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the vc effort will focus capital-efficient capital-efficient EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.3185171549157342, 'p': 0.5538095238095239, 'r': 0.23127289377289376}, 'rouge-2': {'f': 0.13806837079799644, 'p': 0.2519047619047619, 'r': 0.10163455825220531}, 'rouge-l': {'f': 0.2465749733498619, 'p': 0.5338095238095238, 'r': 0.22571733821733822}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.0981746008119049, 'p': 0.11833333333333333, 'r': 0.09095959595959596}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.08939833997197888, 'p': 0.11833333333333333, 'r': 0.09095959595959596}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the automaker releases capital-efficient to capital-efficient EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS cutting-edge for for EOS \n",
      "epoch: 13, loss: 2.80, time: 1.59 sec, dev loss: 12.15, dev perplexity: 188679.53\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the vc effort will focus capital-efficient capital-efficient EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.4286380442341214, 'p': 0.5989285714285715, 'r': 0.344429181929182}, 'rouge-2': {'f': 0.17158008373370542, 'p': 0.22476190476190477, 'r': 0.1427664655605832}, 'rouge-l': {'f': 0.35010057359916746, 'p': 0.5642063492063492, 'r': 0.32081807081807084}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.08150793461462598, 'p': 0.09416666666666666, 'r': 0.07984848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.07405575905560542, 'p': 0.09416666666666666, 'r': 0.07984848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the singer releases quiz to EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS people . EOS people . . EOS \n",
      "epoch: 14, loss: 2.63, time: 1.60 sec, dev loss: 12.58, dev perplexity: 289257.16\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS capital-efficient EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.49165360786416884, 'p': 0.6576984126984127, 'r': 0.41074481074481073}, 'rouge-2': {'f': 0.25699254668019406, 'p': 0.30317460317460315, 'r': 0.23658645276292334}, 'rouge-l': {'f': 0.429765880091687, 'p': 0.641031746031746, 'r': 0.40305250305250306}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.03555555467061731, 'p': 0.05, 'r': 0.02909090909090909}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.03151391022357248, 'p': 0.05, 'r': 0.02909090909090909}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new singer releases to to EOS EOS to EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS people EOS EOS EOS \n",
      "epoch: 15, loss: 2.48, time: 1.61 sec, dev loss: 12.96, dev perplexity: 425127.38\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS to to EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.45868774037101956, 'p': 0.5928571428571427, 'r': 0.3884157509157509}, 'rouge-2': {'f': 0.23535297799687155, 'p': 0.2592530345471522, 'r': 0.2269584990173225}, 'rouge-l': {'f': 0.4166896066907516, 'p': 0.5928571428571427, 'r': 0.3884157509157509}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.06388888746537427, 'p': 0.08166666666666667, 'r': 0.05484848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.058414733414667255, 'p': 0.08166666666666667, 'r': 0.05484848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the automaker daily quiz to EOS EOS knowledge EOS EOS EOS offensive knowledge EOS EOS people EOS EOS people EOS EOS EOS \n",
      "epoch: 16, loss: 2.36, time: 1.60 sec, dev loss: 13.43, dev perplexity: 676792.12\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS to to EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.5401646476512554, 'p': 0.6654761904761904, 'r': 0.4691849816849817}, 'rouge-2': {'f': 0.28188400607548814, 'p': 0.29811507936507936, 'r': 0.2761093481681717}, 'rouge-l': {'f': 0.48614579326909163, 'p': 0.6420995670995671, 'r': 0.4559371184371185}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.09782233869550075, 'p': 0.11595238095238095, 'r': 0.09234848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.08805427615011605, 'p': 0.11595238095238095, 'r': 0.09234848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the automaker daily test to knowledge to knowledge EOS EOS EOS knowledge knowledge EOS EOS knowledge EOS EOS knowledge EOS EOS EOS \n",
      "epoch: 17, loss: 2.24, time: 1.60 sec, dev loss: 13.97, dev perplexity: 1168722.88\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.5462846172181797, 'p': 0.6992857142857142, 'r': 0.46843711843711844}, 'rouge-2': {'f': 0.3418868207081182, 'p': 0.39714285714285713, 'r': 0.3216887687475922}, 'rouge-l': {'f': 0.49756738925636634, 'p': 0.6992857142857142, 'r': 0.46843711843711844}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.10844016805961494, 'p': 0.12511904761904763, 'r': 0.10345959595959597}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.10040617220829198, 'p': 0.12511904761904763, 'r': 0.10345959595959597}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees daily the to the to knowledge EOS EOS EOS key EOS EOS EOS key EOS EOS key EOS EOS EOS \n",
      "epoch: 18, loss: 2.11, time: 1.64 sec, dev loss: 14.12, dev perplexity: 1358351.38\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.5875734160989206, 'p': 0.7544047619047618, 'r': 0.498992673992674}, 'rouge-2': {'f': 0.349736573270182, 'p': 0.40436507936507937, 'r': 0.32982099599746656}, 'rouge-l': {'f': 0.5268115498671169, 'p': 0.7401190476190476, 'r': 0.4906593406593407}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.09371794633121987, 'p': 0.10761904761904761, 'r': 0.09095959595959596}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0862130505151958, 'p': 0.10761904761904761, 'r': 0.09095959595959596}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees designed the to the to ad EOS EOS EOS italian EOS EOS EOS EOS EOS EOS italian EOS EOS EOS \n",
      "epoch: 19, loss: 2.00, time: 1.69 sec, dev loss: 14.39, dev perplexity: 1774556.25\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.6543495746138821, 'p': 0.8039285714285714, 'r': 0.5672619047619047}, 'rouge-2': {'f': 0.3984965386909507, 'p': 0.45310245310245306, 'r': 0.37455616279145687}, 'rouge-l': {'f': 0.5903037960796029, 'p': 0.7729761904761905, 'r': 0.550595238095238}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.1019444420486459, 'p': 0.11873015873015873, 'r': 0.10005050505050506}, 'rouge-2': {'f': 0.009090908694214892, 'p': 0.016666666666666666, 'r': 0.00625}, 'rouge-l': {'f': 0.09224358974336092, 'p': 0.11873015873015873, 'r': 0.10005050505050506}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees daily the the the to people EOS EOS EOS people EOS EOS EOS people EOS EOS people EOS EOS EOS \n",
      "epoch: 20, loss: 1.91, time: 1.70 sec, dev loss: 14.81, dev perplexity: 2699995.25\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS EOS EOS \n",
      "rouge {'rouge-1': {'f': 0.6655887848404147, 'p': 0.8160173160173161, 'r': 0.5773351648351648}, 'rouge-2': {'f': 0.4025484312161821, 'p': 0.45019841269841276, 'r': 0.3829720932662109}, 'rouge-l': {'f': 0.6011915846332414, 'p': 0.7892316017316018, 'r': 0.5606684981684982}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.08924824986284517, 'p': 0.10523809523809524, 'r': 0.08893939393939393}, 'rouge-2': {'f': 0.009090908694214892, 'p': 0.016666666666666666, 'r': 0.00625}, 'rouge-l': {'f': 0.07914722111065403, 'p': 0.10523809523809524, 'r': 0.08893939393939393}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the signees daily the the the to people EOS EOS EOS people people EOS EOS people EOS EOS people EOS EOS EOS \n",
      "epoch: 21, loss: 1.82, time: 1.69 sec, dev loss: 15.03, dev perplexity: 3381544.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS people people \n",
      "rouge {'rouge-1': {'f': 0.6361563583869867, 'p': 0.7902777777777777, 'r': 0.5480616605616606}, 'rouge-2': {'f': 0.39457679295636755, 'p': 0.4408333333333334, 'r': 0.3752797855739032}, 'rouge-l': {'f': 0.5772318525141378, 'p': 0.7759920634920634, 'r': 0.5397283272283272}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.08663419726579473, 'p': 0.10107142857142856, 'r': 0.08893939393939393}, 'rouge-2': {'f': 0.009090908694214892, 'p': 0.016666666666666666, 'r': 0.00625}, 'rouge-l': {'f': 0.07560116310096973, 'p': 0.10107142857142856, 'r': 0.08893939393939393}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the singer daily the the the to people EOS EOS EOS people people EOS EOS people EOS EOS people with allergies gluten \n",
      "epoch: 22, loss: 1.77, time: 1.70 sec, dev loss: 15.35, dev perplexity: 4636791.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS . EOS EOS EOS EOS EOS EOS EOS gluten gluten gluten \n",
      "rouge {'rouge-1': {'f': 0.6976910359979935, 'p': 0.8009848484848486, 'r': 0.6358211233211233}, 'rouge-2': {'f': 0.43337484116705716, 'p': 0.44574675324675317, 'r': 0.44080037609449374}, 'rouge-l': {'f': 0.6566740882633036, 'p': 0.7898737373737375, 'r': 0.62748778998779}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.07413419726579475, 'p': 0.08107142857142857, 'r': 0.07984848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.06557369057349723, 'p': 0.08107142857142857, 'r': 0.07984848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the singer daily the the the to offensive EOS EOS with offensive offensive with EOS offensive with with offensive with with gluten \n",
      "epoch: 23, loss: 1.68, time: 1.71 sec, dev loss: 15.54, dev perplexity: 5621673.50\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS gluten gluten gluten \n",
      "rouge {'rouge-1': {'f': 0.6870992486495476, 'p': 0.8156204906204907, 'r': 0.620650183150183}, 'rouge-2': {'f': 0.4419887267743777, 'p': 0.4557539682539682, 'r': 0.45308041631571044}, 'rouge-l': {'f': 0.6312875301055425, 'p': 0.7906204906204908, 'r': 0.6039835164835164}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.05686274366129185, 'p': 0.06444444444444444, 'r': 0.05484848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.05267830208995218, 'p': 0.06444444444444444, 'r': 0.05484848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new singer daily the to the to offensive EOS EOS EOS key key . EOS key for for key bobsled for key \n",
      "epoch: 24, loss: 1.64, time: 1.68 sec, dev loss: 15.78, dev perplexity: 7105838.50\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS gluten gluten \n",
      "rouge {'rouge-1': {'f': 0.6827683709595116, 'p': 0.7882950382950382, 'r': 0.6179792429792429}, 'rouge-2': {'f': 0.4690018028247337, 'p': 0.4953446226975638, 'r': 0.45740044269456026}, 'rouge-l': {'f': 0.6426046018074945, 'p': 0.7792041292041291, 'r': 0.6096459096459096}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.046899765515746636, 'p': 0.055952380952380955, 'r': 0.045757575757575754}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.042915076088252624, 'p': 0.055952380952380955, 'r': 0.045757575757575754}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "the singer daily daily knowledge the EOS key EOS for for key key for for key for for key for for key \n",
      "epoch: 25, loss: 1.60, time: 1.69 sec, dev loss: 16.05, dev perplexity: 9323174.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS EOS gluten gluten \n",
      "rouge {'rouge-1': {'f': 0.7278609518409498, 'p': 0.8121320346320345, 'r': 0.6692612942612942}, 'rouge-2': {'f': 0.4737828088224899, 'p': 0.48804917304917295, 'r': 0.4686349598114304}, 'rouge-l': {'f': 0.687187783016528, 'p': 0.7911796536796537, 'r': 0.6512057387057386}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.04217032827747352, 'p': 0.045, 'r': 0.045757575757575754}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.03773547880678036, 'p': 0.045, 'r': 0.045757575757575754}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise automaker daily the gluten the EOS people EOS EOS EOS people people EOS gluten people for for people for for gluten \n",
      "epoch: 26, loss: 1.53, time: 1.68 sec, dev loss: 16.10, dev perplexity: 9841564.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS people people people people \n",
      "rouge {'rouge-1': {'f': 0.7114382930770567, 'p': 0.8038419913419913, 'r': 0.6547313797313796}, 'rouge-2': {'f': 0.543803122826262, 'p': 0.575040515040515, 'r': 0.5327016121133769}, 'rouge-l': {'f': 0.683612052008131, 'p': 0.8038419913419913, 'r': 0.6547313797313796}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.022222221728395074, 'p': 0.025, 'r': 0.02}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.021693121693096163, 'p': 0.025, 'r': 0.02}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise signees daily daily knowledge knowledge EOS people EOS EOS EOS people people EOS EOS people for for people for for gluten \n",
      "epoch: 27, loss: 1.44, time: 1.67 sec, dev loss: 16.01, dev perplexity: 9009844.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS companies EOS EOS EOS EOS EOS EOS EOS people people people \n",
      "rouge {'rouge-1': {'f': 0.7415062805635515, 'p': 0.8175541125541125, 'r': 0.6909493284493284}, 'rouge-2': {'f': 0.5451721893899963, 'p': 0.5701305638805638, 'r': 0.5359338374044256}, 'rouge-l': {'f': 0.7215858942644243, 'p': 0.8175541125541125, 'r': 0.6909493284493284}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.04937154208376176, 'p': 0.050162337662337665, 'r': 0.05484848484848485}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.044431050611451785, 'p': 0.050162337662337665, 'r': 0.05484848484848485}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise signees daily daily knowledge the gluten people EOS EOS EOS people people EOS gluten people EOS EOS people EOS EOS EOS \n",
      "epoch: 28, loss: 1.39, time: 1.70 sec, dev loss: 16.21, dev perplexity: 10929372.00\n",
      "####################################################\n",
      "0\n",
      "########################Train######################\n",
      "BOS new vc effort will focus on capital-efficient energy companies . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "new vc effort will focus on capital-efficient energy companies companies EOS . EOS EOS EOS EOS EOS EOS people people people people \n",
      "rouge {'rouge-1': {'f': 0.7998563305650205, 'p': 0.8721428571428571, 'r': 0.7471459096459097}, 'rouge-2': {'f': 0.6133490505432879, 'p': 0.6361277937748526, 'r': 0.6056308071013953}, 'rouge-l': {'f': 0.7832217599058939, 'p': 0.8721428571428571, 'r': 0.7471459096459097}}\n",
      "#############Dev###########\n",
      "rouge {'rouge-1': {'f': 0.053475934876261844, 'p': 0.06666666666666667, 'r': 0.04727272727272727}, 'rouge-2': {'f': 0.00869565175047261, 'p': 0.014285714285714285, 'r': 0.00625}, 'rouge-l': {'f': 0.04833458761346503, 'p': 0.06666666666666667, 'r': 0.04727272727272727}}\n",
      "BOS the UNK 's UNK was held at UNK UNK wednesday at his UNK UNK , UNK UNK to people EOS PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "advertise signees daily daily the the to people EOS EOS . people people for gluten people . . people EOS EOS EOS \n",
      "epoch: 29, loss: 1.28, time: 1.69 sec, dev loss: 16.29, dev perplexity: 11922701.00\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['hidden_size'] = 200\n",
    "params['emb_size'] = 1024\n",
    "params['num_layers'] = 1\n",
    "params['batch_size'] = 50\n",
    "params['vocab_size'] = len(word2ind)\n",
    "params['learning_rate'] = 0.01\n",
    "params['epochs'] = 30\n",
    "net = SuperAwesome(params)\n",
    "\n",
    "train_lm(data['train'], params, net, fscore = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV6FMt4iOGsr"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_lm(data['test'][0:20], net, bsz=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exract_abstract_elmo.ipynb",
   "provenance": [
    {
     "file_id": "1b1FEXCe4z1wxaRraJ4W9v1SiTLjvqv3x",
     "timestamp": 1556476567559
    },
    {
     "file_id": "18Ye9CF1feEqECdVEiTa-Tnn_sIsZ-TRX",
     "timestamp": 1556255796462
    },
    {
     "file_id": "1SNNZDOXYYZ4mkbMBW6yTghgkTb9CY6fh",
     "timestamp": 1554915703195
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
